{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PyM8Vm0pc-GE"
   },
   "source": [
    "## 1. Entorno crudeTanksEnv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NAwj1foPc-GF"
   },
   "source": [
    "### 1.1. Establecer el entorno"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WjnIDGG5c-GG"
   },
   "source": [
    "En primer lugar cargaremos la librería __gym__ e inicializaremos el entorno."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not running on CoLab\n"
     ]
    }
   ],
   "source": [
    "if not ('isColab' in locals()):\n",
    "    if 'google.colab' in str(get_ipython()):\n",
    "        print('Running on CoLab')\n",
    "        !pip install gymnasium\n",
    "        !wget https://github.com/BaltiBoix/Scheduling/raw/master/tankEnvV1.py\n",
    "        isColab = True\n",
    "    else:\n",
    "        print('Not running on CoLab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "44GG2cz6c-GH",
    "outputId": "7bed56d4-3b93-48a0-c112-fbe4e24d5ca2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\rl\\lib\\site-packages\\gymnasium\\utils\\passive_env_checker.py:31: UserWarning: \u001b[33mWARN: A Box observation space has an unconventional shape (neither an image, nor a 1D vector). We recommend flattening the observation to have only a 1D vector or use a custom policy to properly process the data. Actual observation shape: (2, 10)\u001b[0m\n",
      "  logger.warn(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\rl\\lib\\site-packages\\gymnasium\\utils\\passive_env_checker.py:31: UserWarning: \u001b[33mWARN: A Box observation space has an unconventional shape (neither an image, nor a 1D vector). We recommend flattening the observation to have only a 1D vector or use a custom policy to properly process the data. Actual observation shape: (2, 5)\u001b[0m\n",
      "  logger.warn(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\rl\\lib\\site-packages\\gymnasium\\utils\\passive_env_checker.py:31: UserWarning: \u001b[33mWARN: A Box observation space has an unconventional shape (neither an image, nor a 1D vector). We recommend flattening the observation to have only a 1D vector or use a custom policy to properly process the data. Actual observation shape: (7, 10)\u001b[0m\n",
      "  logger.warn(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\rl\\lib\\site-packages\\gymnasium\\utils\\passive_env_checker.py:31: UserWarning: \u001b[33mWARN: A Box observation space has an unconventional shape (neither an image, nor a 1D vector). We recommend flattening the observation to have only a 1D vector or use a custom policy to properly process the data. Actual observation shape: (7, 5)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium.spaces import flatten as flat\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy, copy\n",
    "from collections import namedtuple, deque, OrderedDict\n",
    "import os\n",
    "import IPython.display\n",
    "import sklearn\n",
    "import sklearn.pipeline\n",
    "from sklearn.kernel_approximation import RBFSampler\n",
    "\n",
    "import tankEnvV1\n",
    "\n",
    "env = gym.envs.make('crudeTanksEnv-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lkjAHGpcc-GI",
    "outputId": "1a0d0493-6e4c-4f1d-ed94-07b95ef040e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is torch cuda available?: True\n"
     ]
    }
   ],
   "source": [
    "print('Is torch cuda available?: {}'.format(torch.cuda.is_available()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "swTbkl9EEFOI",
    "outputId": "80a7a1c6-83bd-4af6-b2e6-cea6ba6bf9b0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EnvSpec(id='crudeTanksEnv-v1', entry_point=<class 'tankEnvV1.crudeTanksEnv'>, reward_threshold=500, nondeterministic=False, max_episode_steps=720, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='crudeTanksEnv', version=1)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "163"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flat(env.observation_space, env.observation_space.sample()).shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LVFHNO4QEFOJ"
   },
   "source": [
    "### Agente Dueling DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bUTaCzvGEFOJ"
   },
   "source": [
    "En este apartado resolveremos el mismo entorno con las mismas características para el agente, pero usando una dueling DQN. Como en el caso anterior, primero definiremos el modelo de red neuronal, luego describiremos el comportamiento del agente, lo entrenaremos y, finalmente, testearemos el funcionamiento del mismo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3PbnwP0zEFOJ"
   },
   "source": [
    "### Definición de la arquitectura  de la red neuronal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XTx4eQ1XEFOK"
   },
   "source": [
    "El objetivo principal de las dueling DQN es \"ahorrarse\" el cálculo del valor de Q en aquéllos estados en los que es irrelevante la acción que se tome. Para ello se descompone la función Q en dos componentes:\n",
    "\n",
    "\n",
    "$$Q(s, a) = A(s, a) + V (s)$$\n",
    "\n",
    "\n",
    "Esta descomposición se realiza a nivel de la arquitectura de la red neuronal. Las primeras capas que teníamos en la DQN serán comunes, y luego la red se dividirá en dos partes separadas definidas por el resto de capas.\n",
    "\n",
    "La descomposición en sub-redes del modelo de la DQN implementada en el apartado anterior, será entonces:\n",
    "<ol>\n",
    "    <li>Bloque común, la red CNN</li>\n",
    "    <li>Red advantage A(s,a):</li>\n",
    "         <ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "XLfykmyeEFOK"
   },
   "outputs": [],
   "source": [
    "class duelingDQN(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, env, learning_rate=1e-3, device='cuda'):\n",
    "        \"\"\"\n",
    "        Params\n",
    "        ======\n",
    "        input_shape: tamaño del espacio de estados\n",
    "        n_outputs: tamaño del espacio de acciones\n",
    "        actions: array de acciones posibles\n",
    "        device: cpu o cuda\n",
    "        red_cnn: definición de la red convolucional\n",
    "        value: definición de la red lineal value\n",
    "        advantage: definición de la red lineal advantage\n",
    "        \"\"\"\n",
    "\n",
    "        super(duelingDQN, self).__init__()\n",
    "\n",
    "        if device=='cuda' and torch.cuda.is_available():\n",
    "            self.device = 'cuda'\n",
    "        else:\n",
    "            self.device = 'cpu'\n",
    "        self.n_inputs = flat(env.observation_space, env.observation_space.sample()).shape[0]\n",
    "        self.n_outputs = env.action_space.n\n",
    "        self.actions = np.arange(env.action_space.n)\n",
    "        self.learning_rate = learning_rate\n",
    "            \n",
    "\n",
    "        self.red_lineal = torch.nn.Sequential(\n",
    "            torch.nn.Linear(self.n_inputs, 128, bias=True),\n",
    "            torch.nn.ReLU(),        \n",
    "            torch.nn.Linear(128, 256, bias=True),\n",
    "            torch.nn.ReLU()        \n",
    "        )\n",
    "        \n",
    "        self.advantage = torch.nn.Sequential(\n",
    "            torch.nn.Linear(256, 256, bias=True),\n",
    "            torch.nn.ReLU(),        \n",
    "            torch.nn.Linear(256, self.n_outputs, bias=True)\n",
    "        )\n",
    "        \n",
    "        self.value = torch.nn.Sequential(\n",
    "            torch.nn.Linear(256, 256, bias=True),\n",
    "            torch.nn.ReLU(),        \n",
    "            torch.nn.Linear(256, 1, bias=True)\n",
    "        )\n",
    "\n",
    "        ### Se ofrece la opción de trabajar con cuda\n",
    "        if self.device == 'cuda':\n",
    "            self.red_lineal.cuda()\n",
    "            self.value.cuda()\n",
    "            self.advantage.cuda()\n",
    "\n",
    "        ### Inicializamos el optimizador\n",
    "        self.optimizer = torch.optim.RAdam(self.parameters(), lr=learning_rate)\n",
    "        self.lr_sched = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, mode='max', patience=25000,\n",
    "                                                                   factor=0.5, min_lr=0.00001, verbose=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.red_lineal(x)\n",
    "        adv = self.advantage(out)\n",
    "        out = self.value(out) + adv\n",
    "        return out\n",
    "\n",
    "    ### Método e-greedy\n",
    "    def get_action(self, state, actionAvail, epsilon=0.05):\n",
    "        if np.random.random() < epsilon:\n",
    "            for _ in range(10):\n",
    "                action = np.random.choice(self.actions)  # acción aleatoria\n",
    "                if actionAvail[action]:\n",
    "                    break\n",
    "        else:\n",
    "            qvals = self.get_qvals(state)  # acción a partir del cálculo del valor de Q para esa acción\n",
    "            action= torch.max(qvals * torch.tensor(actionAvail+1E-5, device=self.device), dim=-1)[1].item()\n",
    "        return action\n",
    "\n",
    "\n",
    "    def get_qvals(self, state):\n",
    "        if type(state) is tuple:\n",
    "            state = np.array(state)\n",
    "        state_t = torch.FloatTensor(state).to(device=self.device)\n",
    "        return self.forward(state_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KbzUgqBKc-GP"
   },
   "source": [
    "####  _Buffer_ de repetición de experiencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "9F0QBdxTZKJl"
   },
   "outputs": [],
   "source": [
    "class experienceReplayBuffer:\n",
    "\n",
    "    def __init__(self, memory_size=50000, burn_in=10000):\n",
    "        self.memory_size = memory_size\n",
    "        self.burn_in = burn_in\n",
    "        self.buffer = namedtuple('Buffer', \n",
    "            field_names=['state', 'action', 'reward', 'done', 'next_state'])\n",
    "        self.replay_memory = deque(maxlen=memory_size)\n",
    "\n",
    "    ##Creamos una lista de índices aleatorios y empaquetamos las experiencias en arrays de Numpy (facilita el cálculo posterior de la pérdida)\n",
    "    def sample_batch(self, batch_size=32):\n",
    "        samples = np.random.choice(len(self.replay_memory), batch_size, \n",
    "                                   replace=False)\n",
    "        # Use asterisk operator to unpack deque \n",
    "        batch = zip(*[self.replay_memory[i] for i in samples])\n",
    "        return batch\n",
    "\n",
    "    ## Se añaden las nuevas experiencias \n",
    "    def append(self, state, action, reward, done, next_state):\n",
    "        self.replay_memory.append(\n",
    "            self.buffer(state, action, reward, done, next_state))\n",
    "\n",
    "    ## Rellenamos el buffer con experiencias aleatorias al inicio del entrenamiento\n",
    "    def burn_in_capacity(self):\n",
    "        return len(self.replay_memory) / self.burn_in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I6IZsEAjEFOL"
   },
   "source": [
    "### Definición del agente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "vcRx0IkjEFOL"
   },
   "outputs": [],
   "source": [
    "class duelingDQNAgent:\n",
    "\n",
    "    def __init__(self, env, main_network,\n",
    "                 buffer,\n",
    "                 epsilon=0.1, eps_decay=0.99, batch_size=32, nblock=100):\n",
    "        \"\"\"\"\n",
    "        Params\n",
    "        ======\n",
    "        env: entorno\n",
    "        dnnetwork: clase con la red neuronal diseñada\n",
    "        target_network: red objetivo\n",
    "        buffer: clase con el buffer de repetición de experiencias\n",
    "        epsilon: epsilon\n",
    "        eps_decay: epsilon decay\n",
    "        batch_size: batch size\n",
    "        nblock: bloque de los X últimos episodios de los que se calculará la media de recompensa\n",
    "        reward_threshold: umbral de recompensa definido en el entorno\n",
    "        \"\"\"\n",
    "        if torch.cuda.is_available():\n",
    "            self.device = 'cuda'\n",
    "        else:\n",
    "            self.device = 'cpu'\n",
    "        self.env = env\n",
    "        self.main_network = main_network\n",
    "        self.target_network = deepcopy(main_network) # red objetivo (copia de la principal)\n",
    "        self.buffer = buffer\n",
    "        self.epsilon = epsilon\n",
    "        self.eps_decay = eps_decay\n",
    "        self.batch_size = batch_size\n",
    "        self.nblock = nblock\n",
    "        self.reward_threshold = env.spec.reward_threshold\n",
    "        self.mean_rewards = -10000.\n",
    "        self.update_loss = []\n",
    "        self.training_rewards = []\n",
    "        self.mean_training_rewards = []\n",
    "        self.epsilons = []\n",
    "        self.losses = []\n",
    "        self.initialize()\n",
    "\n",
    "    def initialize(self):\n",
    "        self.total_reward = 0\n",
    "        self.step_count = 0\n",
    "        state, data = self.env.reset()\n",
    "        self.state0 = flat(self.env.observation_space, state)\n",
    "        self.actionAvail = data['actionAvail']\n",
    "\n",
    "    def take_step(self, eps, mode='train'):\n",
    "        if mode == 'explore':\n",
    "            action = self.env.action_space.sample()  # acción aleatoria en el burn-in\n",
    "        else:\n",
    "            action = self.main_network.get_action(self.state0, self.actionAvail, eps)# acción a partir del valor de Q (elección de la acción con mejor Q)\n",
    "        self.step_count += 1\n",
    "\n",
    "        new_state, reward, done, truncated, info = self.env.step(action)\n",
    "        new_state = flat(env.observation_space, new_state)\n",
    "        self.total_reward += reward\n",
    "        self.buffer.append(self.state0, action, reward, done, new_state) # guardar experiencia en el buffer\n",
    "        self.state0 = deepcopy(new_state)\n",
    "        self.actionAvail = info['actionAvail']\n",
    "\n",
    "#         if done:\n",
    "#             state, data = self.env.reset()\n",
    "#             self.state0 = flat(self.env.observation_space, state)\n",
    "#             self.actionAvail = data['actionAvail']\n",
    "        return done\n",
    "\n",
    "\n",
    "\n",
    "    ## Entrenamiento\n",
    "    def train(self, gamma=0.99, max_episodes=50000,\n",
    "              batch_size=32,\n",
    "              dnn_update_frequency=4,\n",
    "              dnn_sync_frequency=2000, min_episodios=250, min_epsilon = 0.01):\n",
    "        self.gamma = gamma\n",
    "        # Rellenamos el buffer con N experiencias aleatorias ()\n",
    "        print(\"Filling replay buffer...\")\n",
    "        while self.buffer.burn_in_capacity() < 1:\n",
    "            self.take_step(self.epsilon, mode='explore')\n",
    "\n",
    "        episode = 0\n",
    "        training = True\n",
    "        print(\"Training...\")\n",
    "        maximo = -10000.0\n",
    "        while training:\n",
    "            state, data = self.env.reset()\n",
    "            self.state0 = flat(self.env.observation_space, state)\n",
    "            self.actionAvail = data['actionAvail']\n",
    "            self.total_reward = 0\n",
    "            gamedone = False\n",
    "            while gamedone == False:\n",
    "                # El agente toma una acción\n",
    "                gamedone = self.take_step(self.epsilon, mode='train')\n",
    "\n",
    "                if self.step_count % dnn_update_frequency == 0:\n",
    "                    self.update()\n",
    "\n",
    "                if self.step_count % dnn_sync_frequency == 0:\n",
    "                    self.target_network.load_state_dict(\n",
    "                        self.main_network.state_dict())\n",
    "\n",
    "            episode += 1\n",
    "            self.epsilons.append(self.epsilon)\n",
    "            if len(self.update_loss):\n",
    "                self.losses.append(self.update_loss[-1])\n",
    "            self.training_rewards.append(self.total_reward) # guardamos las recompensas obtenidas\n",
    "            if self.total_reward > maximo:\n",
    "                maximo = self.total_reward\n",
    "                self.maxS = deepcopy(self.env.S)\n",
    "\n",
    "            self.update_loss = []\n",
    "\n",
    "            self.mean_rewards = np.mean(self.training_rewards[-self.nblock:])\n",
    "            self.mean_training_rewards.append(self.mean_rewards)                    \n",
    "\n",
    "            if self.epsilon == min_epsilon and self.mean_rewards < self.mean_training_rewards[-self.nblock]:\n",
    "                self.epsilon = 0.5\n",
    "                print(\"reinitialize epsilon\")\n",
    "\n",
    "            if episode > 100 and episode % 100 == 0:\n",
    "                self.plot_rewards()\n",
    "\n",
    "            print(\"\\rEpisode {:d} Mean Rewards {:.2f} Epsilon {:.4f} , Maximo {:.2f}\\t\\t\".format(\n",
    "                episode, self.mean_rewards, self.epsilon, maximo), end=\"\")\n",
    "\n",
    "            # Comprobar si se ha llegado al máximo de episodios\n",
    "            if episode >= max_episodes:\n",
    "                training = False\n",
    "                print('\\nEpisode limit reached.')\n",
    "                break\n",
    "\n",
    "\n",
    "            # Termina el juego si la media de recompensas ha llegado al umbral fijado para este juego\n",
    "            # y se ha entrenado un mínimo de episodios\n",
    "            if self.mean_rewards >= self.reward_threshold and min_episodios <  episode:\n",
    "                training = False\n",
    "                print('\\nEnvironment solved in {} episodes!'.format(\n",
    "                    episode))\n",
    "                break\n",
    "\n",
    "            self.epsilon =  max(self.epsilon * self.eps_decay, min_epsilon)\n",
    "\n",
    "\n",
    "     ## Cálculo de la pérdida\n",
    "    def calculate_loss(self, batch):\n",
    "        # Separamos las variables de la experiencia y las convertimos a tensores\n",
    "        states, actions, rewards, dones, next_states = [i for i in batch]\n",
    "        rewards_vals = torch.FloatTensor(rewards).to(device=self.device).reshape(-1,1)\n",
    "        actions_vals = torch.LongTensor(np.array(actions)).reshape(-1,1).to(\n",
    "            device=self.device)\n",
    "        dones_t = torch.BoolTensor(dones).to(device=self.device)\n",
    "\n",
    "        # Obtenemos los valores de Q de la red principal\n",
    "        qvals = torch.gather(self.main_network.get_qvals(states), 1, actions_vals)\n",
    "\n",
    "        #DQN update#\n",
    "        next_actions = torch.max(self.main_network.get_qvals(next_states), dim=-1)[1]\n",
    "        next_actions_vals = next_actions.reshape(-1,1).to(device=self.device)\n",
    "        # Obtenemos los valores de Q de la red objetivo\n",
    "        target_qvals = self.target_network.get_qvals(next_states)\n",
    "        qvals_next = torch.gather(target_qvals, 1, next_actions_vals) #.detach()\n",
    "\n",
    "        qvals_next[dones_t] = 0 # 0 en estados terminales\n",
    "\n",
    "        expected_qvals = self.gamma * qvals_next + rewards_vals\n",
    "        \n",
    "        loss = torch.nn.MSELoss()(qvals, expected_qvals.reshape(-1,1))\n",
    "        return loss\n",
    "\n",
    "\n",
    "\n",
    "    def update(self):\n",
    "        self.main_network.optimizer.zero_grad()  # eliminamos cualquier gradiente pasado\n",
    "        batch = self.buffer.sample_batch(batch_size=self.batch_size) # seleccionamos un conjunto del buffer\n",
    "        loss = self.calculate_loss(batch) # calculamos la pérdida\n",
    "        loss.backward() # hacemos la diferencia para obtener los gradientes\n",
    "        self.main_network.optimizer.step() # aplicamos los gradientes a la red neuronal\n",
    "        self.main_network.lr_sched.step(self.mean_rewards)\n",
    "        # Guardamos los valores de pérdida\n",
    "        if self.device == 'cuda':\n",
    "            self.update_loss.append(loss.detach().cpu().numpy())\n",
    "        else:\n",
    "            self.update_loss.append(loss.detach().numpy())\n",
    "\n",
    "    def plot_rewards(self):\n",
    "      \n",
    "        IPython.display.clear_output(wait=True)\n",
    "\n",
    "        plt.figure(figsize=(12,8))\n",
    "        plt.plot(self.training_rewards, label='Rewards')\n",
    "        plt.plot(self.mean_training_rewards, label='Mean Rewards')\n",
    "        plt.axhline(self.env.spec.reward_threshold, color='r', label=\"Reward threshold\")\n",
    "        plt.xlabel('Episodes')\n",
    "        plt.ylabel('Rewards')\n",
    "        plt.legend(loc=\"upper left\")\n",
    "        plt.grid()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0r4yT6PkEFOM"
   },
   "source": [
    "### 3.3 Entrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_LIX-FFmEFOM"
   },
   "source": [
    "A continuación entrenaremos el modelo dueling DQN con los mismos hiperparámetros con los que entrenamos la DQN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "5a7oXEKAEFON"
   },
   "outputs": [],
   "source": [
    "lr = 0.001            #Velocidad de aprendizaje\n",
    "BATCH_SIZE = 32       #Conjunto a coger del buffer para la red neuronal\n",
    "MEMORY_SIZE = 10000   #Máxima capacidad del buffer\n",
    "GAMMA = 0.99          #Valor gamma de la ecuación de Bellman\n",
    "EPSILON = 1           #Valor inicial de epsilon\n",
    "EPSILON_DECAY = .99   #Decaimiento de epsilon\n",
    "MIN_EPSILON = 0.02    #Mínimo epsilon\n",
    "NBLOCK = 100          #Número de steps para rellenar el buffer\n",
    "MAX_EPISODES = 5000   #Número máximo de episodios (el agente debe aprender antes de llegar a este valor)\n",
    "MIN_EPISODIOS = 100   #Número minímo de episodios\n",
    "BURN_IN = 2000        #Número de episodios iniciales usados para rellenar el buffer antes de entrenar\n",
    "DNN_UPD = 1           #Frecuencia de actualización de la red neuronal \n",
    "DNN_SYNC = 1500       #Frecuencia de sincronización de pesos entre la red neuronal y la red objetivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "mul_dr3REFON"
   },
   "outputs": [],
   "source": [
    "buffer = experienceReplayBuffer(memory_size=MEMORY_SIZE, burn_in=BURN_IN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "UZXhVLsQEFOO"
   },
   "outputs": [],
   "source": [
    "ddqn = duelingDQN(env, learning_rate=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "hEQrao6gEFOO"
   },
   "outputs": [],
   "source": [
    "dagent = duelingDQNAgent(env, ddqn, buffer, EPSILON, EPSILON_DECAY, BATCH_SIZE, NBLOCK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z0DJPyAaEFOO",
    "outputId": "6cd4b7d1-ba4a-49d3-daa1-04f64d433e73"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filling replay buffer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\rl\\lib\\site-packages\\gymnasium\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "dagent.train(gamma=GAMMA, max_episodes=MAX_EPISODES, \n",
    "             batch_size=BATCH_SIZE, dnn_update_frequency=DNN_UPD, \n",
    "             dnn_sync_frequency=DNN_SYNC, min_episodios=MIN_EPISODIOS, min_epsilon=MIN_EPSILON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 977
    },
    "id": "SoCj_0v0EFOP",
    "outputId": "6d167427-f06f-4da9-dce4-3f8d7edc923c"
   },
   "outputs": [],
   "source": [
    "dagent.plot_rewards()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dagent.maxS.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "emxTlQu1eTfV"
   },
   "outputs": [],
   "source": [
    "myfile = 'agentDDQN_Trained_Model.pth'\n",
    "if os.path.isfile(myfile):\n",
    "    os.remove(myfile)\n",
    "torch.save(ddqn.state_dict(), myfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2n7hw5DPfCTb",
    "outputId": "8b7ffef8-d71d-481c-c645-323800eb50fb"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "env = gym.envs.make('crudeTanksEnv-v1')\n",
    "\n",
    "d_dqn = duelingDQN(env, device='cuda')\n",
    "d_dqn.load_state_dict(torch.load(myfile))\n",
    "\n",
    "reward_list = []\n",
    "n_list = []\n",
    "for n_episodes in range(20):\n",
    "    state, data = env.reset()\n",
    "    state0 = flat(env.observation_space, state)\n",
    "    actionAvail = data['actionAvail']\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "    n = 0\n",
    "    action_list = []\n",
    "    while not done:\n",
    "        action= torch.max(d_dqn.get_qvals(state0) * torch.tensor(actionAvail+1E-5, device=d_dqn.device), dim=-1)[1].item()\n",
    "        action_list.append(action)\n",
    "        state, reward, done, truncated, info = env.step(action)\n",
    "        actionAvail = info['actionAvail']\n",
    "        episode_reward += reward\n",
    "        n += 1\n",
    "        state0 = flat(env.observation_space, state)\n",
    "    a, b = np.unique(action_list, return_counts=True)\n",
    "    txt = ' '.join(['{:2d}: {:3d} |'.format(x, y) for x, y in zip(a, b)])\n",
    "    print('{:2d} n: {:4d}   reward: {:4.0f}   Action Freq {}'.format(n_episodes+1, n, episode_reward, txt))\n",
    "    n_list.append(n)\n",
    "    reward_list.append(episode_reward)\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 370
    },
    "id": "KUJ1m2glhnM7",
    "outputId": "57db154a-7079-42d6-ed6c-34bbeb68ec58"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "\n",
    "fig.suptitle('Recompensa de 20 partidas con el modelo ajustado')\n",
    "\n",
    "ax.plot(reward_list)\n",
    "ax.axhline(y=env.spec.reward_threshold, color='r', linestyle='-')\n",
    "ax.set_xlabel('suma de recompensas de cada partida')\n",
    "ax.set_xticks(range(20))\n",
    "ax.grid()\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
