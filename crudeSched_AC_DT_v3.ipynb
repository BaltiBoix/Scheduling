{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E04Z1oI_c-GD"
   },
   "source": [
    "# Práctica\n",
    "\n",
    "Se utilizará el <i>framework</i> de __Pytorch__. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PyM8Vm0pc-GE"
   },
   "source": [
    "## 1. Entorno Crude Scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NAwj1foPc-GF"
   },
   "source": [
    "### 1.1. Establecer el entorno"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WjnIDGG5c-GG"
   },
   "source": [
    "En primer lugar cargaremos la librería __gym__ e inicializaremos el entorno."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "gather": {
     "logged": 1676910028788
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not running on CoLab\n"
     ]
    }
   ],
   "source": [
    "if not ('isColab' in locals()):\n",
    "    if 'google.colab' in str(get_ipython()):\n",
    "        print('Running on CoLab')\n",
    "        !pip install gymnasium\n",
    "        !wget https://github.com/BaltiBoix/Scheduling/raw/master/tankEnv.py\n",
    "        isColab = True\n",
    "    else:\n",
    "        print('Not running on CoLab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "gather": {
     "logged": 1676910030894
    },
    "id": "44GG2cz6c-GH",
    "outputId": "0400139c-300c-4b2d-997e-f1cf26008a53"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages/gymnasium/utils/passive_env_checker.py:31: UserWarning: \u001b[33mWARN: A Box observation space has an unconventional shape (neither an image, nor a 1D vector). We recommend flattening the observation to have only a 1D vector or use a custom policy to properly process the data. Actual observation shape: (2, 10)\u001b[0m\n",
      "  logger.warn(\n",
      "/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages/gymnasium/utils/passive_env_checker.py:31: UserWarning: \u001b[33mWARN: A Box observation space has an unconventional shape (neither an image, nor a 1D vector). We recommend flattening the observation to have only a 1D vector or use a custom policy to properly process the data. Actual observation shape: (2, 5)\u001b[0m\n",
      "  logger.warn(\n",
      "/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages/gymnasium/utils/passive_env_checker.py:31: UserWarning: \u001b[33mWARN: A Box observation space has an unconventional shape (neither an image, nor a 1D vector). We recommend flattening the observation to have only a 1D vector or use a custom policy to properly process the data. Actual observation shape: (7, 10)\u001b[0m\n",
      "  logger.warn(\n",
      "/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages/gymnasium/utils/passive_env_checker.py:31: UserWarning: \u001b[33mWARN: A Box observation space has an unconventional shape (neither an image, nor a 1D vector). We recommend flattening the observation to have only a 1D vector or use a custom policy to properly process the data. Actual observation shape: (7, 5)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium.spaces import flatten as flat\n",
    "import torch\n",
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy, copy\n",
    "from collections import namedtuple, deque, OrderedDict\n",
    "import os\n",
    "import IPython.display\n",
    "import sklearn\n",
    "import sklearn.pipeline\n",
    "from sklearn.kernel_approximation import RBFSampler\n",
    "\n",
    "import tankEnvV2\n",
    "\n",
    "env = gym.envs.make('crudeTanksEnv-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "gather": {
     "logged": 1676910031031
    },
    "id": "lkjAHGpcc-GI",
    "outputId": "96a1758f-e83e-4e28-d99a-82be5ea1b2f1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is torch cuda available?: False\n"
     ]
    }
   ],
   "source": [
    "print('Is torch cuda available?: {}'.format(torch.cuda.is_available()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descripción del entorno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "gather": {
     "logged": 1676910031415
    },
    "id": "zxWyoDnXtyC9",
    "outputId": "0317685d-00a1-40a1-b894-2da8893e788b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EnvSpec(id='crudeTanksEnv-v0', entry_point=<class 'tankEnv.crudeTanksEnv'>, reward_threshold=500, nondeterministic=False, max_episode_steps=720, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='crudeTanksEnv', version=0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatFixData(env, t=0, nCargos=6):\n",
    "    n = nCargos - len(env.S.cargo.sched.keys())\n",
    "    if n > 0:\n",
    "        fSched = n*12*[0.0]\n",
    "    else:\n",
    "        fSched = []\n",
    "    for k, v in env.S.cargo.sched.items():\n",
    "        fSched += [k-t] + [v.vol] + v.toDict()['comp']\n",
    "    fAssay = env.S.unit.assay.reshape(-1)\n",
    "    return np.concatenate((fSched, fAssay, env.S.unit.volCutMax), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state0, data = env.reset()\n",
    "state0 = np.concatenate((flatFixData(env, env.S.t), data['actionAvail'], flat(env.observation_space, state0)), axis=-1)\n",
    "n_inputs = state0.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crudeNames = np.array(['ARABIA LIGERO', 'BONNY LIGERO', 'BRASS RIVER', 'BRENT', 'CPC BLEND', 'DALIA', \n",
    "                       'PAZFLOR', 'SARIR', 'SIRTICA', 'ZAFIRO BLEND'])\n",
    "\n",
    "assay = np.array(\n",
    "    [[0.2083, 0.2503, 0.3046, 0.286 , 0.3979, 0.0596, 0.0843, 0.2039, 0.3133, 0.1962],\n",
    "    [0.1639, 0.1502, 0.17  , 0.1457, 0.187 , 0.1094, 0.0978, 0.1249, 0.1546, 0.1427],\n",
    "    [0.22  , 0.3191, 0.289 , 0.2297, 0.2281, 0.253 , 0.2845, 0.232, 0.2341, 0.2863],\n",
    "    [0.2209, 0.2094, 0.1862, 0.2114, 0.1351, 0.3135, 0.2804, 0.2541, 0.1819, 0.2339],\n",
    "    [0.1869, 0.071 , 0.0502, 0.1272, 0.0519, 0.2645, 0.253 , 0.1851, 0.1161, 0.1409]]\n",
    ")\n",
    "        \n",
    "unitVolCutMax = np.array([0.25, 0.2, 0.4, 0.3, 0.2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_UA4SPt96-tH"
   },
   "source": [
    "## Actor-Critic method\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PmpmuawyAr66"
   },
   "source": [
    "Basado en el ejemplo de pytorch:  \n",
    "\n",
    "https://medium.com/geekculture/actor-critic-implementing-actor-critic-methods-82efb998c273"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "gather": {
     "logged": 1676910037894
    },
    "id": "1tbKMQI-6-tU"
   },
   "outputs": [],
   "source": [
    "class AC_actor(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, env, n_inputs, learning_rate=1e-3, device='cpu'):\n",
    "        \"\"\"\n",
    "        Actor ANN\n",
    "        \"\"\"\n",
    "        super(AC_actor, self).__init__()\n",
    "\n",
    "        self.n_inputs = n_inputs\n",
    "        self.n_outputs = env.action_space.n\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        if torch.cuda.is_available() and device=='cuda':\n",
    "            self.device = 'cuda'\n",
    "        else:\n",
    "            self.device = 'cpu'\n",
    "\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        # AC Actor\n",
    "        self.actor = torch.nn.Sequential(\n",
    "            torch.nn.Linear(self.n_inputs, 512, bias=True),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(512, 512, bias=True),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(512, 512, bias=True),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(512, 512, bias=True),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(512, 256, bias=True),\n",
    "            torch.nn.ReLU(),       \n",
    "            torch.nn.Linear(256, 256, bias=True),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(256, 128, bias=True),\n",
    "            torch.nn.ReLU(),       \n",
    "            torch.nn.Linear(128, self.n_outputs, bias=True),\n",
    "            torch.nn.Softmax(dim=-1)\n",
    "        )\n",
    "\n",
    "        if self.device == 'cuda':\n",
    "            self.actor.cuda()\n",
    "        \n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "        # self.lr_sched = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, mode='max', \n",
    "        #                                                            patience=200, factor=0.5, min_lr=0.0001, verbose=True)\n",
    "\n",
    "        # self.init_weights()\n",
    "\n",
    "    #Obtención de las probabilidades de las posibles acciones\n",
    "    def get_action(self, state):\n",
    "        state_t = torch.FloatTensor(state).to(device=self.device)\n",
    "        \n",
    "        return torch.distributions.Categorical(self.actor(state_t))\n",
    "\n",
    "    def init_weights(self):\n",
    "        self.actor.apply(self.init_weights_)\n",
    "\n",
    "    def init_weights_(self, m):\n",
    "        if isinstance(m, torch.nn.Linear):\n",
    "            torch.nn.init.xavier_uniform_(m.weight)\n",
    "            m.bias.data.fill_(pow(5., -0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "gather": {
     "logged": 1676910038057
    },
    "id": "pcVFk2oB-Idp"
   },
   "outputs": [],
   "source": [
    "class AC_critic(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, env, n_inputs, learning_rate=1e-3, device='cpu'):\n",
    "        \"\"\"\n",
    "        Critic ANN\n",
    "        \"\"\"\n",
    "        super(AC_critic, self).__init__()\n",
    "\n",
    "        self.n_inputs = n_inputs\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        if torch.cuda.is_available() and device=='cuda':\n",
    "            self.device = 'cuda'\n",
    "        else:\n",
    "            self.device = 'cpu'\n",
    "\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        # AC Critic\n",
    "        self.critic = torch.nn.Sequential(\n",
    "            torch.nn.Linear(self.n_inputs, 512, bias=True),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(512, 512, bias=True),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(512, 512, bias=True),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(512, 512, bias=True),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(512, 256, bias=True),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(256, 128, bias=True),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(128, 1, bias=True)\n",
    "        )\n",
    "\n",
    "        if self.device == 'cuda':\n",
    "            self.critic.cuda()\n",
    "        \n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "        # self.lr_sched = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, mode='max', \n",
    "        #                                                            patience=200, factor=0.5, min_lr=0.00001)\n",
    "    \n",
    "    def get_val(self, state):\n",
    "        state_t = torch.FloatTensor(state).to(device=self.device)\n",
    "        return self.critic(state_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1psxD6vR6-tV"
   },
   "source": [
    "\n",
    "### Definición del agente\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "gather": {
     "logged": 1676910038401
    },
    "id": "e1iFeZ8T6-tV"
   },
   "outputs": [],
   "source": [
    "class ACAgent:\n",
    "\n",
    "    def __init__(self, env, assay, unitVolCutMax, seed, dnnet_actor, dnnet_critic, nblock=100):\n",
    "        \"\"\"\n",
    "        Params\n",
    "        ======\n",
    "        env: entorno\n",
    "        dnnetwork: clase con la red neuronal diseñada\n",
    "        nblock: bloque de los X últimos episodios de los que se calculará la media de recompensa\n",
    "        reward_threshold: umbral de recompensa definido en el entorno\n",
    "        \"\"\"\n",
    "        self.env = env\n",
    "        self.assay = assay\n",
    "        self.unitVolCutMax = unitVolCutMax\n",
    "        self.seed = seed\n",
    "        self.dnnet_actor = dnnet_actor\n",
    "        self.dnnet_critic = dnnet_critic\n",
    "        self.device = dnnet_actor.device\n",
    "        self.nblock = nblock\n",
    "        self.reward_threshold = env.spec.reward_threshold\n",
    "        self.initialize()\n",
    "\n",
    "    def initialize(self):\n",
    "        self.losses = []\n",
    "        self.update_loss = []\n",
    "        self.training_rewards = []\n",
    "        self.mean_training_rewards = []\n",
    "    ######\n",
    "\n",
    "    ## Entrenamiento\n",
    "    def train(self, gamma=0.99, max_episodes=2000):\n",
    "        self.gamma = gamma\n",
    "\n",
    "        episode = 0\n",
    "        self.mean_rewards = -1000.0\n",
    "        self.maxRewards = 0.0\n",
    "        training = True\n",
    "        epsilon_max = 0.25\n",
    "        epsilon_min = 0.00\n",
    "        epsilon = epsilon_max\n",
    "        print(\"Training...\")\n",
    "        while training:\n",
    "            state, data = self.env.reset(seed=self.seed, assay=self.assay, unitVolCutMax=self.unitVolCutMax)\n",
    "            actionAvail = self.data['actionAvail']\n",
    "            state0 = np.concatenate((flatFixData(self.env), actionAvail, flat(self.env.observation_space, state)), axis=-1)\n",
    "            episode_rewards = []\n",
    "            gamedone = False\n",
    "            t = 1\n",
    "            I = 1\n",
    "            while gamedone == False:\n",
    "                Prob = self.dnnet_actor.get_action(state0)\n",
    "                action = Prob.sample()\n",
    "                \n",
    "                val = self.dnnet_critic.get_val(state0)\n",
    "                \n",
    "                new_state, reward, gamedone, _, info = self.env.step(action)\n",
    "                actionAvail = info['actionAvail']\n",
    "                new_state = np.concatenate((flatFixData(self.env), actionAvail, flat(self.env.observation_space, new_state)), axis=-1)\n",
    "                \n",
    "                # Almacenamos experiencias que se van obteniendo en este episodio\n",
    "                episode_rewards.append(reward)\n",
    "                \n",
    "                reward_t = torch.tensor([reward], device=self.device, dtype=torch.float)\n",
    "                new_val = self.dnnet_critic.get_val(new_state)\n",
    "                #if terminal state, next state val is 0\n",
    "                if gamedone:\n",
    "                    new_val = torch.tensor([0], device=self.device, dtype=torch.float)\n",
    "                    #if t < 700:\n",
    "                    #    print(action, actionAvail)\n",
    "                \n",
    "                #calculate value function loss with MSE\n",
    "                advantage = reward_t + self.gamma * new_val - val\n",
    "                loss_critic = advantage.square()\n",
    "                loss_critic *= I\n",
    "                \n",
    "                #calculate policy loss\n",
    "                loss_actor = -prob_log * advantage.detach()\n",
    "                loss_actor *= I\n",
    "                \n",
    "#                 if t % 100 == 0:\n",
    "#                     print(cProb.log_prob(cAction), d1Prob.log_prob(d1Action), d2Prob.log_prob(d2Action))\n",
    "                \n",
    "                #Backpropagate policy\n",
    "                self.dnnet_actor.optimizer.zero_grad()\n",
    "                loss_actor.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(self.dnnet_actor.parameters(), 10.0) \n",
    "\n",
    "                #Backpropagate value\n",
    "                self.dnnet_critic.optimizer.zero_grad()\n",
    "                loss_critic.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(self.dnnet_critic.parameters(), 10.0) \n",
    "\n",
    "                nActor, nCritic = self.norm2()\n",
    "                if t > 1:\n",
    "                    nActorMean = (nActorMean*t+nActor)/(t+1)\n",
    "                    nCriticMean = (nCriticMean*t+nCritic)/(t+1)                    \n",
    "                else:\n",
    "                    nActorMean = nActor\n",
    "                    nCriticMean = nCritic\n",
    "                \n",
    "                self.dnnet_actor.optimizer.step()\n",
    "                # self.dnnet_actor.lr_sched.step(self.mean_rewards)\n",
    "\n",
    "                self.dnnet_critic.optimizer.step()\n",
    "                # self.dnnet_critic.lr_sched.step(self.mean_rewards)\n",
    "\n",
    "                state0 = deepcopy(new_state)\n",
    "                I *= self.gamma\n",
    "                t += 1\n",
    "                \n",
    "\n",
    "            episode += 1\n",
    "\n",
    "            self.training_rewards.append(sum(episode_rewards)) # guardamos las recompensas obtenidas\n",
    "            self.mean_rewards = np.mean(self.training_rewards[-self.nblock:])\n",
    "            self.mean_training_rewards.append(self.mean_rewards)\n",
    "\n",
    "            if self.training_rewards[-1] > self.maxRewards:\n",
    "                self.maxRewards = self.training_rewards[-1]\n",
    "                self.maxS = deepcopy(env.S)\n",
    "            \n",
    "            # Comprobamos que todavía quedan episodios\n",
    "            if episode >= max_episodes:\n",
    "                training = False\n",
    "                print('\\nEpisode limit reached.')\n",
    "                break\n",
    "\n",
    "            epsilon = max(epsilon*0.99, epsilon_min)\n",
    "\n",
    "            if episode > 100 and episode % 100 == 0:\n",
    "                self.plot_rewards()\n",
    "\n",
    "            print(\"\\rEpisode {:3d} Mean Rewards {:.2f} Last Reward {:.2f}  n_steps {:3d} Epsilon {:.3f} \"\n",
    "                \"nActor {:.3f} nCritic {:.3f}\\t\\t\".format(\n",
    "                episode, self.mean_rewards, np.mean(self.training_rewards[-1]), t, epsilon, \n",
    "                nActorMean, nCriticMean), end=\"\")\n",
    "\n",
    "            # Termina el juego si la media de recompensas ha llegado al umbral fijado para este juego\n",
    "            if self.mean_rewards >= self.reward_threshold and episode > self.nblock:\n",
    "                training = False\n",
    "                print('\\nEnvironment solved in {} episodes!'.format(\n",
    "                    episode))\n",
    "                break\n",
    "\n",
    "    def plot_rewards(self):\n",
    "       \n",
    "        IPython.display.clear_output(wait=True)\n",
    "\n",
    "        plt.figure(figsize=(12,8))\n",
    "        plt.plot(self.training_rewards, label='Rewards')\n",
    "        plt.plot(self.mean_training_rewards, label='Mean Rewards')\n",
    "        plt.axhline(self.env.spec.reward_threshold, color='r', label=\"Reward threshold\")\n",
    "        plt.xlabel('Episodes')\n",
    "        plt.ylabel('Rewards')\n",
    "        plt.legend(loc=\"upper left\")\n",
    "        plt.grid()\n",
    "        plt.show()\n",
    "\n",
    "    def norm2(self):\n",
    "        norm = lambda parameters : torch.norm(torch.stack([torch.norm(p.grad.detach(), 2) for p in parameters]), 2)\n",
    "        pActor = self.dnnet_actor.parameters()\n",
    "        pCritic = self.dnnet_critic.parameters()\n",
    "        return norm(pActor), norm(pCritic)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eYRA7DMb6-tX"
   },
   "source": [
    "### Entrenamiento\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "gather": {
     "logged": 1676910038694
    },
    "id": "iL8iV_kM6-tX"
   },
   "outputs": [],
   "source": [
    "lr_actor = 1.E-5     #Velocidad de aprendizaje   0.005\n",
    "lr_critic = 5.E-4    #Velocidad de aprendizaje   0.005\n",
    "GAMMA = 0.99         #Valor gamma de la ecuación de Bellman\n",
    "NBLOCK = 100         #Número de steps para rellenar el buffer\n",
    "MAX_EPISODES = 5000  #Número máximo de episodios (el agente debe aprender antes de llegar a este valor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "gather": {
     "logged": 1676910042558
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "396"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stateShape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "gather": {
     "logged": 1676910062409
    },
    "id": "UTAxECge6-tX"
   },
   "outputs": [],
   "source": [
    "ACa = AC_actor(env, n_inputs, learning_rate=lr_actor, device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "gather": {
     "logged": 1676910080374
    },
    "id": "ctVGWP0tAxGK"
   },
   "outputs": [],
   "source": [
    "ACc = AC_critic(env, n_inputs, learning_rate=lr_critic, device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "gather": {
     "logged": 1676910096546
    },
    "id": "4OEDtMh06-tX"
   },
   "outputs": [],
   "source": [
    "AC_agent = ACAgent(env, assay, unitVolCutMax, 200560, ACa, ACc, NBLOCK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 570
    },
    "id": "uEZfeX5A6-tY",
    "outputId": "af00d6ad-09e8-4084-bb94-6dabfe3f9835"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Episode 102 Mean Rewards -77.57 Last Reward -76.05  n_steps 720 Epsilon 0.090 nActor 10.000 nCritic 0.008\t\t\r",
      "Episode 197 Mean Rewards -74.21 Last Reward -71.95  n_steps 720 Epsilon 0.035 nActor 0.014 nCritic 0.005\t\t"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "AC_agent.train(gamma=GAMMA, max_episodes=MAX_EPISODES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1676909942012
    }
   },
   "outputs": [],
   "source": [
    "AC_agent.maxS.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1676909942029
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "AC_agent.maxS.prodsLog[300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 497
    },
    "gather": {
     "logged": 1676909942044
    },
    "id": "M0MQcI2Q6-tY",
    "outputId": "c4b48a88-5eca-4123-90b4-131eca6ed810"
   },
   "outputs": [],
   "source": [
    "AC_agent.plot_rewards()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1676909942060
    },
    "id": "V1uA1JgE6-tZ"
   },
   "outputs": [],
   "source": [
    "myfile = 'agentACDT_Trained_Model.pth'\n",
    "if os.path.isfile(myfile):\n",
    "    os.remove(myfile)\n",
    "torch.save(ACa.state_dict(), myfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z2iikyMV6-tZ",
    "outputId": "11179bc9-eff6-4047-b78c-a42a2dbea1d8",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "env = gym.envs.make(\"crudeTanksEnv-v0\")\n",
    "\n",
    "AC = AC_actor(env, 512, learning_rate=0.005, device='cuda')\n",
    "AC.load_state_dict(torch.load(myfile))\n",
    "\n",
    "state0 = flat(env.observation_space, env.reset()[0])\n",
    "done = False\n",
    "episode_reward = 0\n",
    "n = 0\n",
    "action_list = []\n",
    "while not done:\n",
    "    cProb, d1Prob, d2Prob = AC.get_action(state0)\n",
    "    cAction = cProb.sample()\n",
    "    cAction = torch.clamp(cAction, min=0.1, max=1.0)\n",
    "    d1Action = d1Prob.sample()\n",
    "    d2Action = d2Prob.sample()\n",
    "    action = OrderedDict({'farmTanks': np.array([d1Action.item(), d2Action.item()]),\n",
    "                          'unitFeed': np.array([cAction.item()])})\n",
    "    state, reward, done, _, _ = env.step(action)\n",
    "    state = flat(env.observation_space, state)\n",
    "    episode_reward += reward\n",
    "    n += 1\n",
    "    state0 = deepcopy(state)\n",
    "    if n >= env.spec.max_episode_steps:\n",
    "        done = True\n",
    "\n",
    "print('n: {:4d}   reward: {:4.2f}'.format(n, episode_reward))\n",
    "env.render()\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1676909942080
    }
   },
   "outputs": [],
   "source": [
    "list(AC.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z2iikyMV6-tZ",
    "outputId": "11179bc9-eff6-4047-b78c-a42a2dbea1d8"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "env = gym.envs.make(\"crudeTanksEnv-v0\")\n",
    "\n",
    "AC = AC_actor(env, 512, learning_rate=0.005, device='cuda')\n",
    "AC.load_state_dict(torch.load(myfile))\n",
    "\n",
    "reward_list = []\n",
    "n_list = []\n",
    "for n_episodes in range(20):\n",
    "    state0 = flat(env.observation_space, env.reset()[0])\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "    n = 0\n",
    "    action_list = []\n",
    "    while not done:\n",
    "        cProb, d1Prob, d2Prob = AC.get_action(state0)\n",
    "        cAction = cProb.sample()\n",
    "        d1Action = d1Prob.sample()\n",
    "        d2Action = d2Prob.sample()\n",
    "        action = OrderedDict({'farmTanks': np.array([d1Action.item(), d2Action.item()]),\n",
    "                              'unitFeed': np.array([cAction.item()])})\n",
    "        state, reward, done, _, _ = env.step(action)\n",
    "        state = flat(env.observation_space, state)\n",
    "        episode_reward += reward\n",
    "        n += 1\n",
    "        state0 = deepcopy(state)\n",
    "        if n >= env.spec.max_episode_steps:\n",
    "            done = True\n",
    "\n",
    "    print('{:2d} n: {:4d}   reward: {:4.2f}'.format(n_episodes+1, n, episode_reward))\n",
    "    reward_list.append(episode_reward)\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 370
    },
    "gather": {
     "logged": 1676909942100
    },
    "id": "HDEwTRct6-tZ",
    "outputId": "93f331cd-fd04-457e-de5b-efca077061bf"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "\n",
    "fig.suptitle('Recompensa de 20 partidas con el modelo ajustado')\n",
    "\n",
    "ax.plot(reward_list)\n",
    "ax.axhline(y=env.spec.reward_threshold, color='r', linestyle='-')\n",
    "ax.set_xlabel('suma de recompensas de cada partida')\n",
    "ax.set_xticks(range(20))\n",
    "ax.grid()\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernel_info": {
   "name": "python38-azureml-pt-tf"
  },
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "microsoft": {
   "host": {
    "AzureML": {
     "notebookHasBeenCompleted": true
    }
   },
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
