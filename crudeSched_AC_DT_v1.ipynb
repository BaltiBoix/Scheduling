{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E04Z1oI_c-GD"
   },
   "source": [
    "# Práctica\n",
    "\n",
    "Se utilizará el <i>framework</i> de __Pytorch__. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PyM8Vm0pc-GE"
   },
   "source": [
    "## 1. Entorno Crude Scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NAwj1foPc-GF"
   },
   "source": [
    "### 1.1. Establecer el entorno"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WjnIDGG5c-GG"
   },
   "source": [
    "En primer lugar cargaremos la librería __gym__ e inicializaremos el entorno."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "gather": {
     "logged": 1676491508332
    },
    "id": "44GG2cz6c-GH",
    "outputId": "0400139c-300c-4b2d-997e-f1cf26008a53"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\rl\\lib\\site-packages\\gymnasium\\utils\\passive_env_checker.py:31: UserWarning: \u001b[33mWARN: A Box observation space has an unconventional shape (neither an image, nor a 1D vector). We recommend flattening the observation to have only a 1D vector or use a custom policy to properly process the data. Actual observation shape: (2, 10)\u001b[0m\n",
      "  logger.warn(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\rl\\lib\\site-packages\\gymnasium\\utils\\passive_env_checker.py:31: UserWarning: \u001b[33mWARN: A Box observation space has an unconventional shape (neither an image, nor a 1D vector). We recommend flattening the observation to have only a 1D vector or use a custom policy to properly process the data. Actual observation shape: (2, 5)\u001b[0m\n",
      "  logger.warn(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\rl\\lib\\site-packages\\gymnasium\\utils\\passive_env_checker.py:31: UserWarning: \u001b[33mWARN: A Box observation space has an unconventional shape (neither an image, nor a 1D vector). We recommend flattening the observation to have only a 1D vector or use a custom policy to properly process the data. Actual observation shape: (7, 10)\u001b[0m\n",
      "  logger.warn(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\rl\\lib\\site-packages\\gymnasium\\utils\\passive_env_checker.py:31: UserWarning: \u001b[33mWARN: A Box observation space has an unconventional shape (neither an image, nor a 1D vector). We recommend flattening the observation to have only a 1D vector or use a custom policy to properly process the data. Actual observation shape: (7, 5)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium.spaces import flatten as flat\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy, copy\n",
    "from collections import namedtuple, deque\n",
    "import os\n",
    "import IPython.display\n",
    "import sklearn\n",
    "import sklearn.pipeline\n",
    "from sklearn.kernel_approximation import RBFSampler\n",
    "\n",
    "from tankEnv import *\n",
    "\n",
    "env = gym.envs.make('crudeTanksEnv-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "gather": {
     "logged": 1676491508476
    },
    "id": "lkjAHGpcc-GI",
    "outputId": "96a1758f-e83e-4e28-d99a-82be5ea1b2f1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is torch cuda available?: True\n"
     ]
    }
   ],
   "source": [
    "print('Is torch cuda available?: {}'.format(torch.cuda.is_available()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descripción del entorno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "gather": {
     "logged": 1676491508623
    },
    "id": "zxWyoDnXtyC9",
    "outputId": "0317685d-00a1-40a1-b894-2da8893e788b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EnvSpec(id='crudeTanksEnv-v0', entry_point=<class 'tankEnv.crudeTanksEnv'>, reward_threshold=850, nondeterministic=False, max_episode_steps=720, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='crudeTanksEnv', version=0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "nQ9nEBi4kenB"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 2, 3])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#if actionAvail[0][action['farmTanks'][0]] and actionAvail[1][action['farmTanks'][1]]:\n",
    "flat(env.action_space, env.action_space.sample())\n",
    "np.append(np.array([1,2]), np.array([2,3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "gather": {
     "logged": 1676491512227
    }
   },
   "outputs": [],
   "source": [
    "class dStat():\n",
    "    def __init__(self, env, seed=None):\n",
    "        self.dState = deque(maxlen=2)\n",
    "        self.env = env\n",
    "        self.seed = seed\n",
    "        return\n",
    "    \n",
    "    def reset(self):\n",
    "        self.dState.clear()\n",
    "        state, data = self.env.reset(seed=self.seed)\n",
    "        self.dState.append(flat(self.env.observation_space, state))\n",
    "        action = self.env.action_space.sample()\n",
    "        while not (data['actionAvail'][0][action['farmTanks'][0]] and data['actionAvail'][1][action['farmTanks'][1]]):\n",
    "            action = self.env.action_space.sample()\n",
    "        self.dState[0] = np.append(self.dState[0], flat(self.env.action_space, action))\n",
    "        state, reward, done, _, info = self.env.step(action)\n",
    "        data['actionAvail'] = info['actionAvail']\n",
    "        self.dState.append(flat(self.env.observation_space, state))\n",
    "        return np.append(*self.dState), data\n",
    "        \n",
    "    def step(self, action):\n",
    "        state, reward, done, truncated, info = self.env.step(action)\n",
    "        self.dState.append(flat(self.env.observation_space, state))\n",
    "        self.dState[0] = np.append(self.dState[0], flat(self.env.action_space, action))\n",
    "        return np.append(*self.dState), reward, done, truncated, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dSt = dStat(env)\n",
    "state, data = dSt.reset()\n",
    "stateShape = state.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_UA4SPt96-tH"
   },
   "source": [
    "## Actor-Critic method\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PmpmuawyAr66"
   },
   "source": [
    "Basado en el ejemplo de pytorch:  \n",
    "\n",
    "https://medium.com/geekculture/actor-critic-implementing-actor-critic-methods-82efb998c273"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "gather": {
     "logged": 1676491512359
    },
    "id": "1tbKMQI-6-tU"
   },
   "outputs": [],
   "source": [
    "class AC_actor(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, env, n_inputs, learning_rate=1e-3, device='cpu'):\n",
    "        \"\"\"\n",
    "        Actor ANN\n",
    "        \"\"\"\n",
    "        super(AC_actor, self).__init__()\n",
    "\n",
    "        self.n_inputs = n_inputs\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        if torch.cuda.is_available() and device=='cuda':\n",
    "            self.device = 'cuda'\n",
    "        else:\n",
    "            self.device = 'cpu'\n",
    "\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        # AC Actor\n",
    "        self.actor = torch.nn.Sequential(\n",
    "            torch.nn.Linear(self.n_inputs, 512, bias=True),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(512, 256, bias=True),\n",
    "            torch.nn.ReLU()       \n",
    "        )\n",
    "        self.actorC = torch.nn.Sequential(\n",
    "            torch.nn.Linear(256, 256, bias=True),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(256, 128, bias=True),\n",
    "            torch.nn.ReLU(),       \n",
    "            torch.nn.Linear(128, 1, bias=True),\n",
    "            torch.nn.Softplus()\n",
    "        )\n",
    "        self.actorD1 = torch.nn.Sequential(\n",
    "            torch.nn.Linear(256, 256, bias=True),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(256, 128, bias=True),\n",
    "            torch.nn.ReLU(),       \n",
    "            torch.nn.Linear(128, 13, bias=True),\n",
    "            torch.nn.Softmax(dim=-1)\n",
    "        )\n",
    "        self.actorD2 = torch.nn.Sequential(\n",
    "            torch.nn.Linear(256, 256, bias=True),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(256, 128, bias=True),\n",
    "            torch.nn.ReLU(),       \n",
    "            torch.nn.Linear(128, 6, bias=True),\n",
    "            torch.nn.Softmax(dim=-1)\n",
    "        )\n",
    "\n",
    "        if self.device == 'cuda':\n",
    "            self.actor.cuda()\n",
    "            self.actorC.cuda()\n",
    "            self.actorD1.cuda()\n",
    "            self.actorD2.cuda()\n",
    "        \n",
    "        self.register_parameter(name='lognu', param=torch.nn.Parameter(torch.tensor(0.1).to(device=self.device)))\n",
    "\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "        # self.lr_sched = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, mode='max', \n",
    "        #                                                            patience=200, factor=0.5, min_lr=0.0001, verbose=True)\n",
    "\n",
    "        # self.init_weights()\n",
    "\n",
    "    #Obtención de las probabilidades de las posibles acciones\n",
    "    def get_action(self, state):\n",
    "        state_t = torch.FloatTensor(state).to(device=self.device)\n",
    "        tmp = self.actor(state_t)\n",
    "        \n",
    "        mu = self.actorC(tmp)\n",
    "        if torch.isnan(mu):\n",
    "            mu = torch.tensor(1.0, device=self.device)\n",
    "            print('nan mu')\n",
    "        nu = torch.clamp(self.lognu.exp(), 1e-3, 2)\n",
    "        \n",
    "        return torch.distributions.Normal(mu, nu),\\\n",
    "               torch.distributions.Categorical(self.actorD1(tmp)),\\\n",
    "               torch.distributions.Categorical(self.actorD2(tmp))\n",
    "\n",
    "    def init_weights(self):\n",
    "        self.actor.apply(self.init_weights_)\n",
    "\n",
    "    def init_weights_(self, m):\n",
    "        if isinstance(m, torch.nn.Linear):\n",
    "            torch.nn.init.xavier_uniform_(m.weight)\n",
    "            m.bias.data.fill_(pow(5., -0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "gather": {
     "logged": 1676491512513
    },
    "id": "pcVFk2oB-Idp"
   },
   "outputs": [],
   "source": [
    "class AC_critic(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, env, n_inputs, learning_rate=1e-3, device='cpu'):\n",
    "        \"\"\"\n",
    "        Critic ANN\n",
    "        \"\"\"\n",
    "        super(AC_critic, self).__init__()\n",
    "\n",
    "        self.n_inputs = n_inputs\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        if torch.cuda.is_available() and device=='cuda':\n",
    "            self.device = 'cuda'\n",
    "        else:\n",
    "            self.device = 'cpu'\n",
    "\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        # AC Critic\n",
    "        self.critic = torch.nn.Sequential(\n",
    "            torch.nn.Linear(self.n_inputs, 512, bias=True),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(512, 256, bias=True),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(256, 128, bias=True),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(128, 1, bias=True)\n",
    "        )\n",
    "\n",
    "        if self.device == 'cuda':\n",
    "            self.critic.cuda()\n",
    "        \n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "        # self.lr_sched = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, mode='max', \n",
    "        #                                                            patience=200, factor=0.5, min_lr=0.00001)\n",
    "    \n",
    "    def get_val(self, state):\n",
    "        state_t = torch.FloatTensor(state).to(device=self.device)\n",
    "        return self.critic(state_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1psxD6vR6-tV"
   },
   "source": [
    "\n",
    "### Definición del agente\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "gather": {
     "logged": 1676491512656
    },
    "id": "e1iFeZ8T6-tV"
   },
   "outputs": [],
   "source": [
    "class ACAgent:\n",
    "\n",
    "    def __init__(self, env, dnnet_actor, dnnet_critic, nblock=100):\n",
    "        \"\"\"\n",
    "        Params\n",
    "        ======\n",
    "        env: entorno\n",
    "        dnnetwork: clase con la red neuronal diseñada\n",
    "        nblock: bloque de los X últimos episodios de los que se calculará la media de recompensa\n",
    "        reward_threshold: umbral de recompensa definido en el entorno\n",
    "        \"\"\"\n",
    "        self.env = env\n",
    "        self.dnnet_actor = dnnet_actor\n",
    "        self.dnnet_critic = dnnet_critic\n",
    "        self.device = dnnet_actor.device\n",
    "        self.nblock = nblock\n",
    "        self.reward_threshold = env.spec.reward_threshold\n",
    "        self.initialize()\n",
    "\n",
    "    def initialize(self):\n",
    "        self.losses = []\n",
    "        self.update_loss = []\n",
    "        self.training_rewards = []\n",
    "        self.mean_training_rewards = []\n",
    "    ######\n",
    "\n",
    "    ## Entrenamiento\n",
    "    def train(self, gamma=0.99, max_episodes=2000):\n",
    "        self.gamma = gamma\n",
    "\n",
    "        episode = 0\n",
    "        self.mean_rewards = -1000.0\n",
    "        self.maxRewards = 0.0\n",
    "        training = True\n",
    "        epsilon_max = 0.25\n",
    "        epsilon_min = 0.01\n",
    "        epsilon = epsilon_max\n",
    "        print(\"Training...\")\n",
    "        dSt = dStat(env, seed=200560)\n",
    "        while training:\n",
    "            state0, data = dSt.reset()\n",
    "            actionAvail = data['actionAvail']\n",
    "            episode_rewards = []\n",
    "            gamedone = False\n",
    "            t = 1\n",
    "            I = 1\n",
    "            while gamedone == False:\n",
    "                cProb, d1Prob, d2Prob = self.dnnet_actor.get_action(state0)\n",
    "                cAction = cProb.sample()\n",
    "                cAction = torch.clamp(cAction, 0.1, 1.0)\n",
    "                #d1Prob = torch.distributions.Categorical(d1Prob.probs * torch.tensor(actionAvail[0], device=self.dnnet_actor.device))\n",
    "                #d2Prob = torch.distributions.Categorical(d2Prob.probs * torch.tensor(actionAvail[1], device=self.dnnet_actor.device))\n",
    "                if actionAvail[0].any():\n",
    "                    d1Prob.probs = d1Prob.probs * torch.tensor(actionAvail[0]+1E-5, device=self.dnnet_actor.device)\n",
    "                if actionAvail[1].any():\n",
    "                    d2Prob.probs = d2Prob.probs * torch.tensor(actionAvail[1]+1E-5, device=self.dnnet_actor.device)\n",
    "                d1Action = d1Prob.sample()\n",
    "                d2Action = d2Prob.sample()\n",
    "                if np.random.random() < epsilon:\n",
    "                    action = env.action_space.sample() # acción aleatoria\n",
    "                    while not (actionAvail[0][action['farmTanks'][0]] and actionAvail[1][action['farmTanks'][1]]): \n",
    "                        action = env.action_space.sample() # acción aleatoria\n",
    "                    d1Action = torch.tensor(action['farmTanks'][0], device=self.dnnet_actor.device, dtype=torch.int)\n",
    "                    d2Action = torch.tensor(action['farmTanks'][1], device=self.dnnet_actor.device, dtype=torch.int)\n",
    "                    action['unitFeed'] = np.array([cAction.item()])\n",
    "                    #cAction = torch.tensor(action['unitFeed'], device=self.dnnet_actor.device, dtype=torch.int)\n",
    "                else:\n",
    "                    action = OrderedDict({'farmTanks': np.array([d1Action.item(), d2Action.item()]),\n",
    "                                          'unitFeed': np.array([cAction.item()])})\n",
    "                \n",
    "                val = self.dnnet_critic.get_val(state0)\n",
    "                \n",
    "                prob_log = cProb.log_prob(cAction) + d1Prob.log_prob(d1Action) + d2Prob.log_prob(d2Action)\n",
    "\n",
    "                new_state, reward, gamedone, _, info = dSt.step(action)\n",
    "                actionAvail = info['actionAvail']\n",
    "                \n",
    "                # Almacenamos experiencias que se van obteniendo en este episodio\n",
    "                episode_rewards.append(reward)\n",
    "                \n",
    "                reward_t = torch.tensor([reward], device=self.device, dtype=torch.float)\n",
    "                new_val = self.dnnet_critic.get_val(new_state)\n",
    "                #if terminal state, next state val is 0\n",
    "                if gamedone:\n",
    "                    new_val = torch.tensor([0], device=self.device, dtype=torch.float)\n",
    "                    #if t < 700:\n",
    "                    #    print(action, actionAvail)\n",
    "                \n",
    "                #calculate value function loss with MSE\n",
    "                advantage = reward_t + self.gamma * new_val - val\n",
    "                loss_critic = advantage.square()\n",
    "                loss_critic *= I\n",
    "                \n",
    "                #calculate policy loss\n",
    "                loss_actor = -prob_log * advantage.detach()\n",
    "                loss_actor *= I\n",
    "                \n",
    "#                 if t % 100 == 0:\n",
    "#                     print(cProb.log_prob(cAction), d1Prob.log_prob(d1Action), d2Prob.log_prob(d2Action))\n",
    "                \n",
    "                #Backpropagate policy\n",
    "                self.dnnet_actor.optimizer.zero_grad()\n",
    "                loss_actor.backward()\n",
    "                torch.nn.utils.clip_grad_value_(self.dnnet_actor.parameters(), 10.0) \n",
    "\n",
    "                #Backpropagate value\n",
    "                self.dnnet_critic.optimizer.zero_grad()\n",
    "                loss_critic.backward()\n",
    "                torch.nn.utils.clip_grad_value_(self.dnnet_critic.parameters(), 10.0) \n",
    "\n",
    "                nActor, nCritic = self.norm2()\n",
    "                if t > 0:\n",
    "                    nActorMean = (nActorMean*t+nActor)/(t+1)\n",
    "                    nCriticMean = (nCriticMean*t+nCritic)/(t+1)                    \n",
    "                else:\n",
    "                    nActorMean = nActor\n",
    "                    nCriticMean = nCritic\n",
    "                \n",
    "                self.dnnet_actor.optimizer.step()\n",
    "                # self.dnnet_actor.lr_sched.step(self.mean_rewards)\n",
    "\n",
    "                self.dnnet_critic.optimizer.step()\n",
    "                # self.dnnet_critic.lr_sched.step(self.mean_rewards)\n",
    "\n",
    "                state0 = deepcopy(new_state)\n",
    "                I *= self.gamma\n",
    "                t += 1\n",
    "                \n",
    "\n",
    "            episode += 1\n",
    "\n",
    "            self.training_rewards.append(sum(episode_rewards)) # guardamos las recompensas obtenidas\n",
    "            self.mean_rewards = np.mean(self.training_rewards[-self.nblock:])\n",
    "            self.mean_training_rewards.append(self.mean_rewards)\n",
    "\n",
    "            if self.training_rewards[-1] > self.maxRewards:\n",
    "                self.maxRewards = self.training_rewards[-1]\n",
    "                self.maxS = deepcopy(env.S)\n",
    "            \n",
    "            # Comprobamos que todavía quedan episodios\n",
    "            if episode >= max_episodes:\n",
    "                training = False\n",
    "                print('\\nEpisode limit reached.')\n",
    "                break\n",
    "\n",
    "            epsilon = max(epsilon*0.99, epsilon_min)\n",
    "\n",
    "            if episode > 100 and episode % 100 == 0:\n",
    "                self.plot_rewards()\n",
    "\n",
    "            print(\"\\rEpisode {:3d} Mean Rewards {:.2f} Last Reward {:.2f}  n_steps {:3d} Epsilon {:.3f} \"\n",
    "                \"nActor {:.3f} nCritic {:.3f}\\t\\t\".format(\n",
    "                episode, self.mean_rewards, np.mean(self.training_rewards[-1]), t, epsilon, \n",
    "                nActorMean, nCriticMean), end=\"\")\n",
    "\n",
    "            # Termina el juego si la media de recompensas ha llegado al umbral fijado para este juego\n",
    "            if self.mean_rewards >= self.reward_threshold and episode > self.nblock:\n",
    "                training = False\n",
    "                print('\\nEnvironment solved in {} episodes!'.format(\n",
    "                    episode))\n",
    "                break\n",
    "\n",
    "    def plot_rewards(self):\n",
    "       \n",
    "        IPython.display.clear_output(wait=True)\n",
    "\n",
    "        plt.figure(figsize=(12,8))\n",
    "        plt.plot(self.training_rewards, label='Rewards')\n",
    "        plt.plot(self.mean_training_rewards, label='Mean Rewards')\n",
    "        plt.axhline(self.env.spec.reward_threshold, color='r', label=\"Reward threshold\")\n",
    "        plt.xlabel('Episodes')\n",
    "        plt.ylabel('Rewards')\n",
    "        plt.legend(loc=\"upper left\")\n",
    "        plt.grid()\n",
    "        plt.show()\n",
    "\n",
    "    def norm2(self):\n",
    "        norm = lambda parameters : torch.norm(torch.stack([torch.norm(p.grad.detach(), 2) for p in parameters]), 2)\n",
    "        pActor = self.dnnet_actor.parameters()\n",
    "        pCritic = self.dnnet_critic.parameters()\n",
    "        return norm(pActor), norm(pCritic)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eYRA7DMb6-tX"
   },
   "source": [
    "### Entrenamiento\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "gather": {
     "logged": 1676491512821
    },
    "id": "iL8iV_kM6-tX"
   },
   "outputs": [],
   "source": [
    "lr_actor = 1.E-5     #Velocidad de aprendizaje   0.005\n",
    "lr_critic = 1.E-4    #Velocidad de aprendizaje   0.005\n",
    "GAMMA = 0.99         #Valor gamma de la ecuación de Bellman\n",
    "NBLOCK = 100         #Número de steps para rellenar el buffer\n",
    "MAX_EPISODES = 5000  #Número máximo de episodios (el agente debe aprender antes de llegar a este valor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "gather": {
     "logged": 1676491512988
    },
    "id": "UTAxECge6-tX"
   },
   "outputs": [],
   "source": [
    "ACa = AC_actor(env, stateShape, learning_rate=lr_actor, device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "gather": {
     "logged": 1676491513193
    },
    "id": "ctVGWP0tAxGK"
   },
   "outputs": [],
   "source": [
    "ACc = AC_critic(env, stateShape, learning_rate=lr_critic, device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "gather": {
     "logged": 1676491513323
    },
    "id": "4OEDtMh06-tX"
   },
   "outputs": [],
   "source": [
    "AC_agent = ACAgent(env, ACa, ACc, NBLOCK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 570
    },
    "id": "uEZfeX5A6-tY",
    "outputId": "af00d6ad-09e8-4084-bb94-6dabfe3f9835"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Episode  94 Mean Rewards 320.74 Last Reward 1104.88  n_steps 719 Epsilon 0.097 nActor 3.167 nCritic 1.737\t\t\t\t"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32m<timed eval>:1\u001b[0m\n",
      "Cell \u001b[1;32mIn[26], line 111\u001b[0m, in \u001b[0;36mACAgent.train\u001b[1;34m(self, gamma, max_episodes)\u001b[0m\n\u001b[0;32m    108\u001b[0m loss_critic\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m    109\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_value_(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdnnet_critic\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;241m10.0\u001b[39m) \n\u001b[1;32m--> 111\u001b[0m nActor, nCritic \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm2\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m t \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    113\u001b[0m     nActorMean \u001b[38;5;241m=\u001b[39m (nActorMean\u001b[38;5;241m*\u001b[39mt\u001b[38;5;241m+\u001b[39mnActor)\u001b[38;5;241m/\u001b[39m(t\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[1;32mIn[26], line 181\u001b[0m, in \u001b[0;36mACAgent.norm2\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    179\u001b[0m pActor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdnnet_actor\u001b[38;5;241m.\u001b[39mparameters()\n\u001b[0;32m    180\u001b[0m pCritic \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdnnet_critic\u001b[38;5;241m.\u001b[39mparameters()\n\u001b[1;32m--> 181\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpActor\u001b[49m\u001b[43m)\u001b[49m, norm(pCritic)\n",
      "Cell \u001b[1;32mIn[26], line 178\u001b[0m, in \u001b[0;36mACAgent.norm2.<locals>.<lambda>\u001b[1;34m(parameters)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnorm2\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 178\u001b[0m     norm \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m parameters : torch\u001b[38;5;241m.\u001b[39mnorm(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    179\u001b[0m     pActor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdnnet_actor\u001b[38;5;241m.\u001b[39mparameters()\n\u001b[0;32m    180\u001b[0m     pCritic \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdnnet_critic\u001b[38;5;241m.\u001b[39mparameters()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "AC_agent.train(gamma=GAMMA, max_episodes=MAX_EPISODES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1676492580659
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "scaler.transform([flat(env.observation_space, env.reset()[0])]).shape\n",
    "AC_agent.mean_training_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1676492580681
    }
   },
   "outputs": [],
   "source": [
    "AC_agent.maxS.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1676492580697
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "AC_agent.maxS.prodsLog[300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 497
    },
    "gather": {
     "logged": 1676492580714
    },
    "id": "M0MQcI2Q6-tY",
    "outputId": "c4b48a88-5eca-4123-90b4-131eca6ed810"
   },
   "outputs": [],
   "source": [
    "AC_agent.plot_rewards()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1676492580730
    },
    "id": "V1uA1JgE6-tZ"
   },
   "outputs": [],
   "source": [
    "myfile = 'agentACDT_Trained_Model.pth'\n",
    "if os.path.isfile(myfile):\n",
    "    os.remove(myfile)\n",
    "torch.save(ACa.state_dict(), myfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z2iikyMV6-tZ",
    "outputId": "11179bc9-eff6-4047-b78c-a42a2dbea1d8",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "env = gym.envs.make(\"crudeTanksEnv-v0\")\n",
    "\n",
    "AC = AC_actor(env, 512, learning_rate=0.005, device='cuda')\n",
    "AC.load_state_dict(torch.load(myfile))\n",
    "\n",
    "state0 = flat(env.observation_space, env.reset()[0])\n",
    "done = False\n",
    "episode_reward = 0\n",
    "n = 0\n",
    "action_list = []\n",
    "while not done:\n",
    "    cProb, d1Prob, d2Prob = AC.get_action(state0)\n",
    "    cAction = cProb.sample()\n",
    "    cAction = torch.clamp(cAction, min=0.1, max=1.0)\n",
    "    d1Action = d1Prob.sample()\n",
    "    d2Action = d2Prob.sample()\n",
    "    action = OrderedDict({'farmTanks': np.array([d1Action.item(), d2Action.item()]),\n",
    "                          'unitFeed': np.array([cAction.item()])})\n",
    "    state, reward, done, _, _ = env.step(action)\n",
    "    state = flat(env.observation_space, state)\n",
    "    episode_reward += reward\n",
    "    n += 1\n",
    "    state0 = deepcopy(state)\n",
    "    if n >= env.spec.max_episode_steps:\n",
    "        done = True\n",
    "\n",
    "print('n: {:4d}   reward: {:4.2f}'.format(n, episode_reward))\n",
    "env.render()\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1676492580751
    }
   },
   "outputs": [],
   "source": [
    "list(AC.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z2iikyMV6-tZ",
    "outputId": "11179bc9-eff6-4047-b78c-a42a2dbea1d8"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "env = gym.envs.make(\"crudeTanksEnv-v0\")\n",
    "\n",
    "AC = AC_actor(env, 512, learning_rate=0.005, device='cuda')\n",
    "AC.load_state_dict(torch.load(myfile))\n",
    "\n",
    "reward_list = []\n",
    "n_list = []\n",
    "for n_episodes in range(20):\n",
    "    state0 = flat(env.observation_space, env.reset()[0])\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "    n = 0\n",
    "    action_list = []\n",
    "    while not done:\n",
    "        cProb, d1Prob, d2Prob = AC.get_action(state0)\n",
    "        cAction = cProb.sample()\n",
    "        d1Action = d1Prob.sample()\n",
    "        d2Action = d2Prob.sample()\n",
    "        action = OrderedDict({'farmTanks': np.array([d1Action.item(), d2Action.item()]),\n",
    "                              'unitFeed': np.array([cAction.item()])})\n",
    "        state, reward, done, _, _ = env.step(action)\n",
    "        state = flat(env.observation_space, state)\n",
    "        episode_reward += reward\n",
    "        n += 1\n",
    "        state0 = deepcopy(state)\n",
    "        if n >= env.spec.max_episode_steps:\n",
    "            done = True\n",
    "\n",
    "    print('{:2d} n: {:4d}   reward: {:4.2f}'.format(n_episodes+1, n, episode_reward))\n",
    "    reward_list.append(episode_reward)\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 370
    },
    "gather": {
     "logged": 1676492580770
    },
    "id": "HDEwTRct6-tZ",
    "outputId": "93f331cd-fd04-457e-de5b-efca077061bf"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "\n",
    "fig.suptitle('Recompensa de 20 partidas con el modelo ajustado')\n",
    "\n",
    "ax.plot(reward_list)\n",
    "ax.axhline(y=env.spec.reward_threshold, color='r', linestyle='-')\n",
    "ax.set_xlabel('suma de recompensas de cada partida')\n",
    "ax.set_xticks(range(20))\n",
    "ax.grid()\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernel_info": {
   "name": "python38-azureml-pt-tf"
  },
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "microsoft": {
   "host": {
    "AzureML": {
     "notebookHasBeenCompleted": true
    }
   },
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
