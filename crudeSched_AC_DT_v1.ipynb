{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Práctica\n",
        "\n",
        "Se utilizará el <i>framework</i> de __Pytorch__. "
      ],
      "metadata": {
        "id": "E04Z1oI_c-GD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Entorno Crude Scheduler"
      ],
      "metadata": {
        "id": "PyM8Vm0pc-GE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1. Establecer el entorno"
      ],
      "metadata": {
        "id": "NAwj1foPc-GF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "En primer lugar cargaremos la librería __gym__ e inicializaremos el entorno."
      ],
      "metadata": {
        "id": "WjnIDGG5c-GG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if not ('isColab' in locals()):\r\n",
        "    if 'google.colab' in str(get_ipython()):\r\n",
        "        print('Running on CoLab')\r\n",
        "        !pip install gymnasium\r\n",
        "        !wget https://github.com/BaltiBoix/Scheduling/raw/master/tankEnv.py\r\n",
        "        isColab = True\r\n",
        "    else:\r\n",
        "        print('Not running on CoLab')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Not running on CoLab\n"
        }
      ],
      "execution_count": 1,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1676811558393
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "from gymnasium.spaces import flatten as flat\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from copy import deepcopy, copy\n",
        "from collections import namedtuple, deque, OrderedDict\n",
        "import os\n",
        "import IPython.display\n",
        "import sklearn\n",
        "import sklearn.pipeline\n",
        "from sklearn.kernel_approximation import RBFSampler\n",
        "\n",
        "import tankEnv\n",
        "\n",
        "env = gym.envs.make('crudeTanksEnv-v0')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages/gymnasium/utils/passive_env_checker.py:31: UserWarning: \u001b[33mWARN: A Box observation space has an unconventional shape (neither an image, nor a 1D vector). We recommend flattening the observation to have only a 1D vector or use a custom policy to properly process the data. Actual observation shape: (2, 10)\u001b[0m\n  logger.warn(\n/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages/gymnasium/utils/passive_env_checker.py:31: UserWarning: \u001b[33mWARN: A Box observation space has an unconventional shape (neither an image, nor a 1D vector). We recommend flattening the observation to have only a 1D vector or use a custom policy to properly process the data. Actual observation shape: (2, 5)\u001b[0m\n  logger.warn(\n/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages/gymnasium/utils/passive_env_checker.py:31: UserWarning: \u001b[33mWARN: A Box observation space has an unconventional shape (neither an image, nor a 1D vector). We recommend flattening the observation to have only a 1D vector or use a custom policy to properly process the data. Actual observation shape: (7, 10)\u001b[0m\n  logger.warn(\n/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages/gymnasium/utils/passive_env_checker.py:31: UserWarning: \u001b[33mWARN: A Box observation space has an unconventional shape (neither an image, nor a 1D vector). We recommend flattening the observation to have only a 1D vector or use a custom policy to properly process the data. Actual observation shape: (7, 5)\u001b[0m\n  logger.warn(\n"
        }
      ],
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "gather": {
          "logged": 1676811560666
        },
        "id": "44GG2cz6c-GH",
        "outputId": "0400139c-300c-4b2d-997e-f1cf26008a53"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Is torch cuda available?: {}'.format(torch.cuda.is_available()))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Is torch cuda available?: False\n"
        }
      ],
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "gather": {
          "logged": 1676811560818
        },
        "id": "lkjAHGpcc-GI",
        "outputId": "96a1758f-e83e-4e28-d99a-82be5ea1b2f1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Descripción del entorno"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "env.spec"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 4,
          "data": {
            "text/plain": "EnvSpec(id='crudeTanksEnv-v0', entry_point=<class 'tankEnv.crudeTanksEnv'>, reward_threshold=500, nondeterministic=False, max_episode_steps=720, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='crudeTanksEnv', version=0)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "gather": {
          "logged": 1676811561242
        },
        "id": "zxWyoDnXtyC9",
        "outputId": "0317685d-00a1-40a1-b894-2da8893e788b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#if actionAvail[0][action['farmTanks'][0]] and actionAvail[1][action['farmTanks'][1]]:\n",
        "flat(env.action_space, env.action_space.sample())\n",
        "np.append(np.array([1,2]), np.array([2,3]))"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 5,
          "data": {
            "text/plain": "array([1, 2, 2, 3])"
          },
          "metadata": {}
        }
      ],
      "execution_count": 5,
      "metadata": {
        "id": "nQ9nEBi4kenB",
        "gather": {
          "logged": 1676811562150
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class dStat():\n",
        "    def __init__(self, env, seed=None, scale=True):\n",
        "        self.dState = deque(maxlen=2)\n",
        "        self.env = env\n",
        "        self.seed = seed\n",
        "        self.scale = scale\n",
        "        if self.scale:\n",
        "            self.fitScaler()\n",
        "        else:\n",
        "            self.scaler = None\n",
        "        return\n",
        "    \n",
        "    def reset(self):\n",
        "        self.dState.clear()\n",
        "        state, data = self.env.reset(seed=self.seed)\n",
        "        schedList = []\n",
        "        for k, v in data['sched'].items():\n",
        "            schedList += [k, v.vol, *v.comp]\n",
        "        schedList = np.array(schedList, dtype=float)\n",
        "        self.fixData = np.append(schedList, data['assay'].reshape(-1))\n",
        "        self.dState.append(self.transform(state))\n",
        "        action = self.env.action_space.sample()\n",
        "        while not (data['actionAvail'][0][action['farmTanks'][0]] and data['actionAvail'][1][action['farmTanks'][1]]):\n",
        "            action = self.env.action_space.sample()\n",
        "        self.dState[0] = np.append(self.dState[0], flat(self.env.action_space, action))\n",
        "        state, reward, done, _, info = self.env.step(action)\n",
        "        data['actionAvail'] = info['actionAvail']\n",
        "        self.dState.append(self.transform(state))\n",
        "        state_ = np.append(*self.dState)\n",
        "        #return np.append(self.fixData, state_), data\n",
        "        return state_, data\n",
        "        \n",
        "    def step(self, action):\n",
        "        state, reward, done, truncated, info = self.env.step(action)\n",
        "        self.dState.append(self.transform(state))\n",
        "        self.dState[0] = np.append(self.dState[0], flat(self.env.action_space, action))\n",
        "        state_ = np.append(*self.dState)\n",
        "        #return np.append(self.fixData, state_), reward, done, truncated, info\n",
        "        return state_, reward, done, truncated, info\n",
        "    \n",
        "    def fitScaler(self):\n",
        "        observation_examples = np.array([flat(self.env.observation_space, self.env.observation_space.sample())\\\n",
        "                                         for x in range(10000)])\n",
        "        self.scaler = sklearn.preprocessing.StandardScaler()\n",
        "        self.scaler.fit(observation_examples)\n",
        "        return\n",
        "    \n",
        "    def transform(self, state):\n",
        "        if self.scaler is None:\n",
        "            return flat(self.env.observation_space, state)\n",
        "        return self.scaler.transform([flat(self.env.observation_space, state)])"
      ],
      "outputs": [],
      "execution_count": 6,
      "metadata": {
        "gather": {
          "logged": 1676811563136
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dSt = dStat(env)\n",
        "state, data = dSt.reset()\n",
        "stateShape = state.shape[0]"
      ],
      "outputs": [],
      "execution_count": 7,
      "metadata": {
        "gather": {
          "logged": 1676811567536
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Actor-Critic method\n"
      ],
      "metadata": {
        "id": "_UA4SPt96-tH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Basado en el ejemplo de pytorch:  \n",
        "\n",
        "https://medium.com/geekculture/actor-critic-implementing-actor-critic-methods-82efb998c273"
      ],
      "metadata": {
        "id": "PmpmuawyAr66"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AC_actor(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, env, n_inputs, learning_rate=1e-3, device='cpu'):\n",
        "        \"\"\"\n",
        "        Actor ANN\n",
        "        \"\"\"\n",
        "        super(AC_actor, self).__init__()\n",
        "\n",
        "        self.n_inputs = n_inputs\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "        if torch.cuda.is_available() and device=='cuda':\n",
        "            self.device = 'cuda'\n",
        "        else:\n",
        "            self.device = 'cpu'\n",
        "\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "        # AC Actor\n",
        "        self.actor = torch.nn.Sequential(\n",
        "            torch.nn.Linear(self.n_inputs, 512, bias=True),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(512, 256, bias=True),\n",
        "            torch.nn.ReLU()       \n",
        "        )\n",
        "        self.actorC = torch.nn.Sequential(\n",
        "            torch.nn.Linear(256, 256, bias=True),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(256, 128, bias=True),\n",
        "            torch.nn.ReLU(),       \n",
        "            torch.nn.Linear(128, 1, bias=True),\n",
        "            torch.nn.Softplus()\n",
        "        )\n",
        "        self.actorD1 = torch.nn.Sequential(\n",
        "            torch.nn.Linear(256, 256, bias=True),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(256, 128, bias=True),\n",
        "            torch.nn.ReLU(),       \n",
        "            torch.nn.Linear(128, 13, bias=True),\n",
        "            torch.nn.Softmax(dim=-1)\n",
        "        )\n",
        "        self.actorD2 = torch.nn.Sequential(\n",
        "            torch.nn.Linear(256, 256, bias=True),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(256, 128, bias=True),\n",
        "            torch.nn.ReLU(),       \n",
        "            torch.nn.Linear(128, 6, bias=True),\n",
        "            torch.nn.Softmax(dim=-1)\n",
        "        )\n",
        "\n",
        "        if self.device == 'cuda':\n",
        "            self.actor.cuda()\n",
        "            self.actorC.cuda()\n",
        "            self.actorD1.cuda()\n",
        "            self.actorD2.cuda()\n",
        "        \n",
        "        self.register_parameter(name='lognu', param=torch.nn.Parameter(torch.tensor(0.1).to(device=self.device)))\n",
        "\n",
        "        self.optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
        "        # self.lr_sched = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, mode='max', \n",
        "        #                                                            patience=200, factor=0.5, min_lr=0.0001, verbose=True)\n",
        "\n",
        "        # self.init_weights()\n",
        "\n",
        "    #Obtención de las probabilidades de las posibles acciones\n",
        "    def get_action(self, state):\n",
        "        if state.shape[0] != self.n_inputs:\n",
        "            print(state.shape, state)\n",
        "        state_t = torch.FloatTensor(state).to(device=self.device)\n",
        "        tmp = self.actor(state_t)\n",
        "        \n",
        "        mu = self.actorC(tmp)\n",
        "        if torch.isnan(mu):\n",
        "            mu = torch.tensor(1.0, device=self.device)\n",
        "            print('nan mu')\n",
        "        nu = torch.clamp(self.lognu.exp(), 1e-3, 2)\n",
        "        \n",
        "        return torch.distributions.Normal(mu, nu),\\\n",
        "               torch.distributions.Categorical(self.actorD1(tmp)),\\\n",
        "               torch.distributions.Categorical(self.actorD2(tmp))\n",
        "\n",
        "    def init_weights(self):\n",
        "        self.actor.apply(self.init_weights_)\n",
        "\n",
        "    def init_weights_(self, m):\n",
        "        if isinstance(m, torch.nn.Linear):\n",
        "            torch.nn.init.xavier_uniform_(m.weight)\n",
        "            m.bias.data.fill_(pow(5., -0.5))"
      ],
      "outputs": [],
      "execution_count": 8,
      "metadata": {
        "gather": {
          "logged": 1676811567722
        },
        "id": "1tbKMQI-6-tU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AC_critic(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, env, n_inputs, learning_rate=1e-3, device='cpu'):\n",
        "        \"\"\"\n",
        "        Critic ANN\n",
        "        \"\"\"\n",
        "        super(AC_critic, self).__init__()\n",
        "\n",
        "        self.n_inputs = n_inputs\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "        if torch.cuda.is_available() and device=='cuda':\n",
        "            self.device = 'cuda'\n",
        "        else:\n",
        "            self.device = 'cpu'\n",
        "\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "        # AC Critic\n",
        "        self.critic = torch.nn.Sequential(\n",
        "            torch.nn.Linear(self.n_inputs, 512, bias=True),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(512, 256, bias=True),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(256, 128, bias=True),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(128, 1, bias=True)\n",
        "        )\n",
        "\n",
        "        if self.device == 'cuda':\n",
        "            self.critic.cuda()\n",
        "        \n",
        "        self.optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
        "        # self.lr_sched = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, mode='max', \n",
        "        #                                                            patience=200, factor=0.5, min_lr=0.00001)\n",
        "    \n",
        "    def get_val(self, state):\n",
        "        state_t = torch.FloatTensor(state).to(device=self.device)\n",
        "        return self.critic(state_t)"
      ],
      "outputs": [],
      "execution_count": 9,
      "metadata": {
        "gather": {
          "logged": 1676811568005
        },
        "id": "pcVFk2oB-Idp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Definición del agente\n"
      ],
      "metadata": {
        "id": "1psxD6vR6-tV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ACAgent:\n",
        "\n",
        "    def __init__(self, env, dnnet_actor, dnnet_critic, nblock=100):\n",
        "        \"\"\"\n",
        "        Params\n",
        "        ======\n",
        "        env: entorno\n",
        "        dnnetwork: clase con la red neuronal diseñada\n",
        "        nblock: bloque de los X últimos episodios de los que se calculará la media de recompensa\n",
        "        reward_threshold: umbral de recompensa definido en el entorno\n",
        "        \"\"\"\n",
        "        self.env = env\n",
        "        self.dnnet_actor = dnnet_actor\n",
        "        self.dnnet_critic = dnnet_critic\n",
        "        self.device = dnnet_actor.device\n",
        "        self.nblock = nblock\n",
        "        self.reward_threshold = env.spec.reward_threshold\n",
        "        self.initialize()\n",
        "\n",
        "    def initialize(self):\n",
        "        self.losses = []\n",
        "        self.update_loss = []\n",
        "        self.training_rewards = []\n",
        "        self.mean_training_rewards = []\n",
        "    ######\n",
        "\n",
        "    ## Entrenamiento\n",
        "    def train(self, gamma=0.99, max_episodes=2000):\n",
        "        self.gamma = gamma\n",
        "\n",
        "        episode = 0\n",
        "        self.mean_rewards = -1000.0\n",
        "        self.maxRewards = 0.0\n",
        "        training = True\n",
        "        epsilon_max = 0.25\n",
        "        epsilon_min = 0.00\n",
        "        epsilon = epsilon_max\n",
        "        print(\"Training...\")\n",
        "        dSt = dStat(env, seed=200560)\n",
        "        while training:\n",
        "            state0, data = dSt.reset()\n",
        "            actionAvail = data['actionAvail']\n",
        "            episode_rewards = []\n",
        "            gamedone = False\n",
        "            t = 1\n",
        "            I = 1\n",
        "            while gamedone == False:\n",
        "                cProb, d1Prob, d2Prob = self.dnnet_actor.get_action(state0)\n",
        "                cAction = cProb.sample()\n",
        "                cAction = torch.clamp(cAction, 0.1, 1.0)\n",
        "                #d1Prob = torch.distributions.Categorical(d1Prob.probs * torch.tensor(actionAvail[0], device=self.dnnet_actor.device))\n",
        "                #d2Prob = torch.distributions.Categorical(d2Prob.probs * torch.tensor(actionAvail[1], device=self.dnnet_actor.device))\n",
        "                if actionAvail[0].any():\n",
        "                    d1Prob.probs = d1Prob.probs * torch.tensor(actionAvail[0]+1E-5, device=self.dnnet_actor.device)\n",
        "                if actionAvail[1].any():\n",
        "                    d2Prob.probs = d2Prob.probs * torch.tensor(actionAvail[1]+1E-5, device=self.dnnet_actor.device)\n",
        "                d1Action = d1Prob.sample()\n",
        "                d2Action = d2Prob.sample()\n",
        "                if np.random.random() < epsilon:\n",
        "                    action = env.action_space.sample() # acción aleatoria\n",
        "                    while not (actionAvail[0][action['farmTanks'][0]] and actionAvail[1][action['farmTanks'][1]]): \n",
        "                        action = env.action_space.sample() # acción aleatoria\n",
        "                    d1Action = torch.tensor(action['farmTanks'][0], device=self.dnnet_actor.device, dtype=torch.int)\n",
        "                    d2Action = torch.tensor(action['farmTanks'][1], device=self.dnnet_actor.device, dtype=torch.int)\n",
        "                    action['unitFeed'] = np.array([cAction.item()])\n",
        "                    #cAction = torch.tensor(action['unitFeed'], device=self.dnnet_actor.device, dtype=torch.int)\n",
        "                else:\n",
        "                    action = OrderedDict({'farmTanks': np.array([d1Action.item(), d2Action.item()]),\n",
        "                                          'unitFeed': np.array([cAction.item()])})\n",
        "                \n",
        "                val = self.dnnet_critic.get_val(state0)\n",
        "                \n",
        "                prob_log = cProb.log_prob(cAction) + d1Prob.log_prob(d1Action) + d2Prob.log_prob(d2Action)\n",
        "\n",
        "                new_state, reward, gamedone, _, info = dSt.step(action)\n",
        "                actionAvail = info['actionAvail']\n",
        "                \n",
        "                # Almacenamos experiencias que se van obteniendo en este episodio\n",
        "                episode_rewards.append(reward)\n",
        "                \n",
        "                reward_t = torch.tensor([reward], device=self.device, dtype=torch.float)\n",
        "                new_val = self.dnnet_critic.get_val(new_state)\n",
        "                #if terminal state, next state val is 0\n",
        "                if gamedone:\n",
        "                    new_val = torch.tensor([0], device=self.device, dtype=torch.float)\n",
        "                    #if t < 700:\n",
        "                    #    print(action, actionAvail)\n",
        "                \n",
        "                #calculate value function loss with MSE\n",
        "                advantage = reward_t + self.gamma * new_val - val\n",
        "                loss_critic = advantage.square()\n",
        "                loss_critic *= I\n",
        "                \n",
        "                #calculate policy loss\n",
        "                loss_actor = -prob_log * advantage.detach()\n",
        "                loss_actor *= I\n",
        "                \n",
        "#                 if t % 100 == 0:\n",
        "#                     print(cProb.log_prob(cAction), d1Prob.log_prob(d1Action), d2Prob.log_prob(d2Action))\n",
        "                \n",
        "                #Backpropagate policy\n",
        "                self.dnnet_actor.optimizer.zero_grad()\n",
        "                loss_actor.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(self.dnnet_actor.parameters(), 10.0) \n",
        "\n",
        "                #Backpropagate value\n",
        "                self.dnnet_critic.optimizer.zero_grad()\n",
        "                loss_critic.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(self.dnnet_critic.parameters(), 10.0) \n",
        "\n",
        "                nActor, nCritic = self.norm2()\n",
        "                if t > 1:\n",
        "                    nActorMean = (nActorMean*t+nActor)/(t+1)\n",
        "                    nCriticMean = (nCriticMean*t+nCritic)/(t+1)                    \n",
        "                else:\n",
        "                    nActorMean = nActor\n",
        "                    nCriticMean = nCritic\n",
        "                \n",
        "                self.dnnet_actor.optimizer.step()\n",
        "                # self.dnnet_actor.lr_sched.step(self.mean_rewards)\n",
        "\n",
        "                self.dnnet_critic.optimizer.step()\n",
        "                # self.dnnet_critic.lr_sched.step(self.mean_rewards)\n",
        "\n",
        "                state0 = deepcopy(new_state)\n",
        "                I *= self.gamma\n",
        "                t += 1\n",
        "                \n",
        "\n",
        "            episode += 1\n",
        "\n",
        "            self.training_rewards.append(sum(episode_rewards)) # guardamos las recompensas obtenidas\n",
        "            self.mean_rewards = np.mean(self.training_rewards[-self.nblock:])\n",
        "            self.mean_training_rewards.append(self.mean_rewards)\n",
        "\n",
        "            if self.training_rewards[-1] > self.maxRewards:\n",
        "                self.maxRewards = self.training_rewards[-1]\n",
        "                self.maxS = deepcopy(env.S)\n",
        "            \n",
        "            # Comprobamos que todavía quedan episodios\n",
        "            if episode >= max_episodes:\n",
        "                training = False\n",
        "                print('\\nEpisode limit reached.')\n",
        "                break\n",
        "\n",
        "            epsilon = max(epsilon*0.99, epsilon_min)\n",
        "\n",
        "            if episode > 100 and episode % 100 == 0:\n",
        "                self.plot_rewards()\n",
        "\n",
        "            print(\"\\rEpisode {:3d} Mean Rewards {:.2f} Last Reward {:.2f}  n_steps {:3d} Epsilon {:.3f} \"\n",
        "                \"nActor {:.3f} nCritic {:.3f}\\t\\t\".format(\n",
        "                episode, self.mean_rewards, np.mean(self.training_rewards[-1]), t, epsilon, \n",
        "                nActorMean, nCriticMean), end=\"\")\n",
        "\n",
        "            # Termina el juego si la media de recompensas ha llegado al umbral fijado para este juego\n",
        "            if self.mean_rewards >= self.reward_threshold and episode > self.nblock:\n",
        "                training = False\n",
        "                print('\\nEnvironment solved in {} episodes!'.format(\n",
        "                    episode))\n",
        "                break\n",
        "\n",
        "    def plot_rewards(self):\n",
        "       \n",
        "        IPython.display.clear_output(wait=True)\n",
        "\n",
        "        plt.figure(figsize=(12,8))\n",
        "        plt.plot(self.training_rewards, label='Rewards')\n",
        "        plt.plot(self.mean_training_rewards, label='Mean Rewards')\n",
        "        plt.axhline(self.env.spec.reward_threshold, color='r', label=\"Reward threshold\")\n",
        "        plt.xlabel('Episodes')\n",
        "        plt.ylabel('Rewards')\n",
        "        plt.legend(loc=\"upper left\")\n",
        "        plt.grid()\n",
        "        plt.show()\n",
        "\n",
        "    def norm2(self):\n",
        "        norm = lambda parameters : torch.norm(torch.stack([torch.norm(p.grad.detach(), 2) for p in parameters]), 2)\n",
        "        pActor = self.dnnet_actor.parameters()\n",
        "        pCritic = self.dnnet_critic.parameters()\n",
        "        return norm(pActor), norm(pCritic)\n",
        "        "
      ],
      "outputs": [],
      "execution_count": 10,
      "metadata": {
        "gather": {
          "logged": 1676811568168
        },
        "id": "e1iFeZ8T6-tV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Entrenamiento\n"
      ],
      "metadata": {
        "id": "eYRA7DMb6-tX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lr_actor = 1.E-5     #Velocidad de aprendizaje   0.005\n",
        "lr_critic = 5.E-4    #Velocidad de aprendizaje   0.005\n",
        "GAMMA = 0.99         #Valor gamma de la ecuación de Bellman\n",
        "NBLOCK = 100         #Número de steps para rellenar el buffer\n",
        "MAX_EPISODES = 5000  #Número máximo de episodios (el agente debe aprender antes de llegar a este valor)"
      ],
      "outputs": [],
      "execution_count": 11,
      "metadata": {
        "gather": {
          "logged": 1676811568449
        },
        "id": "iL8iV_kM6-tX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stateShape"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 12,
          "data": {
            "text/plain": "346"
          },
          "metadata": {}
        }
      ],
      "execution_count": 12,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1676811569187
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ACa = AC_actor(env, stateShape, learning_rate=lr_actor, device='cuda')"
      ],
      "outputs": [],
      "execution_count": 13,
      "metadata": {
        "gather": {
          "logged": 1676811570137
        },
        "id": "UTAxECge6-tX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ACc = AC_critic(env, stateShape, learning_rate=lr_critic, device='cuda')"
      ],
      "outputs": [],
      "execution_count": 14,
      "metadata": {
        "gather": {
          "logged": 1676811571104
        },
        "id": "ctVGWP0tAxGK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "AC_agent = ACAgent(env, ACa, ACc, NBLOCK)"
      ],
      "outputs": [],
      "execution_count": 15,
      "metadata": {
        "gather": {
          "logged": 1676811572084
        },
        "id": "4OEDtMh06-tX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "AC_agent.train(gamma=GAMMA, max_episodes=MAX_EPISODES)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Training...\n"
        }
      ],
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 570
        },
        "id": "uEZfeX5A6-tY",
        "outputId": "af00d6ad-09e8-4084-bb94-6dabfe3f9835"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "AC_agent.maxS.render()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1676814446484
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "AC_agent.maxS.prodsLog[300]"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1676814446499
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "AC_agent.plot_rewards()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 497
        },
        "gather": {
          "logged": 1676814446519
        },
        "id": "M0MQcI2Q6-tY",
        "outputId": "c4b48a88-5eca-4123-90b4-131eca6ed810"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "myfile = 'agentACDT_Trained_Model.pth'\n",
        "if os.path.isfile(myfile):\n",
        "    os.remove(myfile)\n",
        "torch.save(ACa.state_dict(), myfile)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1676814446535
        },
        "id": "V1uA1JgE6-tZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "env = gym.envs.make(\"crudeTanksEnv-v0\")\n",
        "\n",
        "AC = AC_actor(env, 512, learning_rate=0.005, device='cuda')\n",
        "AC.load_state_dict(torch.load(myfile))\n",
        "\n",
        "state0 = flat(env.observation_space, env.reset()[0])\n",
        "done = False\n",
        "episode_reward = 0\n",
        "n = 0\n",
        "action_list = []\n",
        "while not done:\n",
        "    cProb, d1Prob, d2Prob = AC.get_action(state0)\n",
        "    cAction = cProb.sample()\n",
        "    cAction = torch.clamp(cAction, min=0.1, max=1.0)\n",
        "    d1Action = d1Prob.sample()\n",
        "    d2Action = d2Prob.sample()\n",
        "    action = OrderedDict({'farmTanks': np.array([d1Action.item(), d2Action.item()]),\n",
        "                          'unitFeed': np.array([cAction.item()])})\n",
        "    state, reward, done, _, _ = env.step(action)\n",
        "    state = flat(env.observation_space, state)\n",
        "    episode_reward += reward\n",
        "    n += 1\n",
        "    state0 = deepcopy(state)\n",
        "    if n >= env.spec.max_episode_steps:\n",
        "        done = True\n",
        "\n",
        "print('n: {:4d}   reward: {:4.2f}'.format(n, episode_reward))\n",
        "env.render()\n",
        "print('\\n')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z2iikyMV6-tZ",
        "outputId": "11179bc9-eff6-4047-b78c-a42a2dbea1d8",
        "scrolled": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "list(AC.parameters())"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1676814446557
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "env = gym.envs.make(\"crudeTanksEnv-v0\")\n",
        "\n",
        "AC = AC_actor(env, 512, learning_rate=0.005, device='cuda')\n",
        "AC.load_state_dict(torch.load(myfile))\n",
        "\n",
        "reward_list = []\n",
        "n_list = []\n",
        "for n_episodes in range(20):\n",
        "    state0 = flat(env.observation_space, env.reset()[0])\n",
        "    done = False\n",
        "    episode_reward = 0\n",
        "    n = 0\n",
        "    action_list = []\n",
        "    while not done:\n",
        "        cProb, d1Prob, d2Prob = AC.get_action(state0)\n",
        "        cAction = cProb.sample()\n",
        "        d1Action = d1Prob.sample()\n",
        "        d2Action = d2Prob.sample()\n",
        "        action = OrderedDict({'farmTanks': np.array([d1Action.item(), d2Action.item()]),\n",
        "                              'unitFeed': np.array([cAction.item()])})\n",
        "        state, reward, done, _, _ = env.step(action)\n",
        "        state = flat(env.observation_space, state)\n",
        "        episode_reward += reward\n",
        "        n += 1\n",
        "        state0 = deepcopy(state)\n",
        "        if n >= env.spec.max_episode_steps:\n",
        "            done = True\n",
        "\n",
        "    print('{:2d} n: {:4d}   reward: {:4.2f}'.format(n_episodes+1, n, episode_reward))\n",
        "    reward_list.append(episode_reward)\n",
        "print('\\n')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z2iikyMV6-tZ",
        "outputId": "11179bc9-eff6-4047-b78c-a42a2dbea1d8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(figsize=(12, 5))\n",
        "\n",
        "fig.suptitle('Recompensa de 20 partidas con el modelo ajustado')\n",
        "\n",
        "ax.plot(reward_list)\n",
        "ax.axhline(y=env.spec.reward_threshold, color='r', linestyle='-')\n",
        "ax.set_xlabel('suma de recompensas de cada partida')\n",
        "ax.set_xticks(range(20))\n",
        "ax.grid()\n",
        "\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        },
        "gather": {
          "logged": 1676814446576
        },
        "id": "HDEwTRct6-tZ",
        "outputId": "93f331cd-fd04-457e-de5b-efca077061bf"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernel_info": {
      "name": "python38-azureml-pt-tf"
    },
    "kernelspec": {
      "name": "python38-azureml-pt-tf",
      "language": "python",
      "display_name": "Python 3.8 - Pytorch and Tensorflow"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.5",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      },
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}